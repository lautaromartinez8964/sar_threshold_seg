{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- ï¼ï¼ï¼ç½‘ç»œä»£ç†è®¾ç½®ï¼ï¼ï¼ ---\n",
    "# åœ¨è¿™é‡Œè®¾ç½®æ‚¨çš„HTTPå’ŒHTTPSä»£ç†ã€‚\n",
    "# è¿™ä¸ªè®¾ç½®åªä¼šå½±å“å½“å‰è¿™ä¸ªPythonè„šæœ¬çš„è¿è¡Œã€‚\n",
    "# è¯·å°† '127.0.0.1:7890' æ›¿æ¢ä¸ºæ‚¨è‡ªå·±çš„ä»£ç†åœ°å€å’Œç«¯å£ã€‚\n",
    "#proxy_url = 'http://127.0.0.1:7897' \n",
    "#os.environ['http_proxy'] = proxy_url\n",
    "#os.environ['https_proxy'] = proxy_url\n",
    "\n",
    "import ee\n",
    "import geemap\n",
    "# ä¸å†éœ€è¦ import sys, import os (é™¤éå…¶ä»–åœ°æ–¹ç”¨åˆ°)\n",
    "# ä¹Ÿä¸å†éœ€è¦ä»»ä½• sys.path.append(...)\n",
    "\n",
    "# åˆå§‹åŒ–GEE\n",
    "ee.Initialize(project='geemap-441216') # æ›¿æ¢ä¸ºæ‚¨çš„é¡¹ç›®ID\n",
    "\n",
    "# Pythonä¼šè‡ªåŠ¨åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸‹å¯»æ‰¾æ¨¡å—ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥å¯¼å…¥ï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5e5f3",
   "metadata": {},
   "source": [
    "# 1.å¯¼å…¥ä¾èµ–-æ•°æ®ç®¡ç†æ¨¡å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle #ç”¨äºåœ¨å›¾è¡¨ä¸­æ·»åŠ å½¢çŠ¶(ä¾‹å¦‚çŸ©å½¢)\n",
    "import re #æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—\n",
    "from ipywidgets import Label #ç”¨äºåˆ›å»ºäº¤äº’å¼æ§ä»¶\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecdcc17",
   "metadata": {},
   "source": [
    "# 2.å®šä¹‰äº¤äº’å¼åœ°å›¾ä»¥é€‰æ‹©æ„Ÿå…´è¶£åŒºåŸŸ-ç•Œé¢ä¸äº¤äº’æ¨¡å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca760c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#è§£æç”¨æˆ·ç‚¹å‡»åœ°å›¾æ—¶ç”Ÿæˆçš„åæ ‡å­—ç¬¦ä¸²ï¼Œæå–ç»çº¬åº¦å¹¶è¿”å›\n",
    "def parseClickedCoordinates(label):\n",
    "  #åˆ©ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œä»label.valueæå–å‡ºåæ ‡ï¼šä¸€ä¸ªå¯èƒ½å¸¦æœ‰è´Ÿå·çš„æµ®ç‚¹æ•° è¿™é‡Œä¸ºã€ç»åº¦ï¼Œçº¬åº¦ã€‘\n",
    "  #æ­£åˆ™è¡¨è¾¾å¼ï¼š r'(?:-)?[0-9]+.[0-9]+'\n",
    "  #r'è¡¨ç¤ºåŸå§‹å­—ç¬¦ï¼Œ(?:-)æ˜¯éæ•è·ç»„ï¼Œ?:è¡¨ç¤ºè´Ÿå·æ˜¯å¯é€‰çš„ï¼Œ[0-9]+è¡¨ç¤ºåŒ¹é…ä¸€ä¸ªæ•°å­—ï¼Œ+è¡¨ç¤ºå¯ä»¥å‡ºç°ä¸€æ¬¡æˆ–å¤šæ¬¡ï¼Œ.åŒ¹é…å°æ•°ç‚¹ï¼Œæœ€åå†æ¬¡åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªæ•°å­—\n",
    "  coords = [float(c) for c in re.findall(r'(?:-)?[0-9]+.[0-9]+', label.value)]\n",
    "  coords.reverse() #åè½¬ä¸ºã€çº¬åº¦ï¼Œç»åº¦ã€‘ï¼Œç¬¦åˆGEEåæ ‡æ ¼å¼\n",
    "  return coords\n",
    "\n",
    "#åˆ›å»ºä¸€ä¸ªLableï¼Œç”¨äºæ˜¾ç¤ºç”¨æˆ·ç‚¹å‡»çš„ç›®æ ‡\n",
    "l = Label()\n",
    "display(1)\n",
    "#å¤„ç†ç”¨æˆ·ä¸åœ°å›¾çš„äº¤äº’äº‹ä»¶ï¼Œå½“ç”¨æˆ·ç‚¹å‡»åœ°å›¾æ—¶ï¼Œå°†ç‚¹å‡»çš„åæ ‡å­˜å‚¨åˆ°Labelæ§ä»¶ä¸­\n",
    "def handle_interaction(**kwargs):\n",
    "  #kwargsåŒ…å«äº¤äº’äº‹ä»¶çš„å‚æ•° kwargs.get('type'):è·å–äº‹ä»¶ç±»å‹ä¸ºé¼ æ ‡ç‚¹å‡» kwargs.get('coordinates')ï¼šè·å–ç‚¹å‡»çš„åæ ‡ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å­˜å‚¨åˆ°Labelæ§ä»¶ä¸­\n",
    "  if kwargs.get('type') == 'click':\n",
    "    l.value = str(kwargs.get('coordinates'))\n",
    "\n",
    "print('è¯·ç‚¹å‡»åœ°å›¾ä»¥é€‰æ‹©ä½ è¦ç›‘æµ‹çš„åŒºåŸŸ')\n",
    "#åˆ›å»ºäº¤äº’å¼åœ°å›¾\n",
    "Map = geemap.Map()\n",
    "Map.on_interaction(handle_interaction)\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa09f9",
   "metadata": {},
   "source": [
    "# 3.å®šä¹‰å‡ ä½•èŒƒå›´å¹¶å±•ç¤º-ç•Œé¢ä¸äº¤äº’æ¨¡å—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e945db95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c521643b",
   "metadata": {},
   "source": [
    "## 1.æ‰‹åŠ¨åˆ’å®šç ”ç©¶åŒºåŸŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon,lat = parseClickedCoordinates(l)\n",
    "w,h = 0.3,0.3 #çŸ©å½¢å®½åº¦ä¸é«˜åº¦ï¼ˆå•ä½ï¼šåº¦ï¼‰\n",
    "\n",
    "geometry = ee.Geometry.Polygon(\n",
    "    [[[lon-w,lat-h],\n",
    "     [lon-w,lat+h],\n",
    "     [lon+w,lat+h],\n",
    "     [lon+w,lat-h]]]\n",
    ")\n",
    "\n",
    "#å°†å‡ ä½•èŒƒå›´æ·»åŠ åˆ°åœ°å›¾\n",
    "Map.addLayer(\n",
    "    geometry,\n",
    "    {'color':'red','fillColor':'00000000'},\n",
    "    'AOI'\n",
    ")\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec44c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE MAP RECTANGLE é€‰æ‹©åŒºåŸŸ\n",
    "#è·å–æ„Ÿå…´è¶£åŒºåŸŸï¼ˆç”¨äºè®ºæ–‡ç»˜å›¾)\n",
    "roi_choose1 = Map.user_roi\n",
    "\n",
    "if roi_choose1 is not None:\n",
    "  #è·å–ROIç±»å‹\n",
    "  roi_type = roi_choose1.type().getInfo()\n",
    "  print(f\"ROI ç±»å‹ï¼š{roi_type}\")\n",
    "\n",
    "  #å¦‚æœæ˜¯Polygonï¼Œè·å–åæ ‡\n",
    "  if roi_type == 'Polygon':\n",
    "    coords = roi_choose1.coordinates().getInfo()\n",
    "    print(f\"ROI åæ ‡ï¼š{coords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3844f95",
   "metadata": {},
   "source": [
    "## 2.ä»geeèµ„äº§é‡Œé¢å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ae0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_id = 'projects/geemap-441216/assets/roi_sci_valencia_small'\n",
    "\n",
    "try:\n",
    "    # ä»GEE AssetsåŠ è½½çŸ¢é‡æ–‡ä»¶ä½œä¸ºFeatureCollection\n",
    "    aoi_fc = ee.FeatureCollection(asset_id)\n",
    "    \n",
    "    \n",
    "    # å¯¹äºæ´ªæ°´åˆ†æï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦çš„æ˜¯æ‰€æœ‰è¦ç´ åˆå¹¶åçš„å‡ ä½•è¾¹ç•Œ\n",
    "    # .geometry() ä¼šå°†FeatureCollectionä¸­æ‰€æœ‰çš„è¦ç´ èåˆæˆä¸€ä¸ªå•ä¸€çš„å‡ ä½•å¯¹è±¡\n",
    "    roi_choose1 = aoi_fc.geometry()\n",
    "    \n",
    "    print(\"æˆåŠŸä»GEE AssetsåŠ è½½ç ”ç©¶åŒºï¼\")\n",
    "    \n",
    "    # åˆ›å»ºä¸€ä¸ªåœ°å›¾æ¥å¯è§†åŒ–ä½ çš„AOIï¼Œç¡®ä¿åŠ è½½æ­£ç¡®\n",
    "    Map = geemap.Map()\n",
    "    Map.addLayer(roi_choose1, {'color': 'red', 'fillColor': '00000000'}, 'Study Area (from Asset)')\n",
    "    Map.centerObject(roi_choose1, 10) # è‡ªåŠ¨ç¼©æ”¾åˆ°ç ”ç©¶åŒº\n",
    "    display(Map)\n",
    "    \n",
    "except ee.EEException as e:\n",
    "    print(f\"åŠ è½½AOIå¤±è´¥ï¼è¯·æ£€æŸ¥ä½ çš„Asset IDæ˜¯å¦æ­£ç¡®: {asset_id}\")\n",
    "    print(f\"é”™è¯¯ä¿¡æ¯: {e}\")\n",
    "    # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œåç»­ä»£ç \n",
    "    raise SystemExit(\"æ— æ³•åŠ è½½AOIï¼Œç¨‹åºç»ˆæ­¢ã€‚\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e42d1",
   "metadata": {},
   "source": [
    "# 4.è¿‡æ»¤Sentinel-æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.è¿‡æ»¤Sentinel-æ•°æ®\n",
    "\n",
    "# æ‰€éœ€æ—¥æœŸ\n",
    "targdate = '2024-10-31'\n",
    "\n",
    "# å°‡æ—¥æœŸéæ¿¾ç¯„åœç¸®å°åˆ°ç›®æ¨™æ—¥æœŸç•¶å¤©\n",
    "start_date = ee.Date(targdate)\n",
    "end_date = start_date.advance(1, 'day')\n",
    "\n",
    "filters = [\n",
    "    ee.Filter.listContains(\"transmitterReceiverPolarisation\",\"VH\"),\n",
    "    ee.Filter.equals(\"instrumentMode\",\"IW\"),\n",
    "    ee.Filter.equals(\"orbitProperties_pass\",\"ASCENDING\"),\n",
    "    ee.Filter.date(start_date, end_date)\n",
    "]\n",
    "\n",
    "# åŠ è¼‰S1æ•¸æ“š\n",
    "s1_collection = ee.ImageCollection(\"COPERNICUS/S1_GRD\") \\\n",
    "    .filter(filters) \\\n",
    "    .filterBounds(roi_choose1)\n",
    "    \n",
    "# æª¢æŸ¥æ˜¯å¦æœ‰å½±åƒæ»¿è¶³æ¢ä»¶\n",
    "s1_size = s1_collection.size().getInfo()\n",
    "print(f\"åœ¨ {targdate} æ‰¾åˆ°æ»¿è¶³æ¢ä»¶çš„ Sentinel-1 å½±åƒæ•¸é‡: {s1_size}\")\n",
    "\n",
    "flood_image = None # å…ˆåˆå§‹åŒ–ç‚º None\n",
    "\n",
    "if s1_size == 0:\n",
    "    print(\"è­¦å‘Šï¼šåœ¨ç›®æ¨™æ—¥æœŸæœªæ‰¾åˆ°æ»¿è¶³æ¢ä»¶çš„å½±åƒï¼å¾ŒçºŒæ­¥é©Ÿå°‡æœƒå¤±æ•—ã€‚\")\n",
    "elif s1_size == 1:\n",
    "    print(\"æ‰¾åˆ° 1 å¼µå½±åƒï¼Œç›´æ¥ä½¿ç”¨ã€‚\")\n",
    "    # [æ ¸å¿ƒä¿®æ­£] ç›´æ¥ç²å–é›†åˆä¸­çš„ç¬¬ä¸€å¼µï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€å¼µï¼‰å½±åƒ\n",
    "    flood_image = ee.Image(s1_collection.first()).clip(roi_choose1)\n",
    "else:\n",
    "    print(f\"è­¦å‘Šï¼šæ‰¾åˆ° {s1_size} å¼µå½±åƒï¼Œå°‡å®ƒå€‘é‘²åµŒï¼ˆmosaicï¼‰æˆä¸€å¼µã€‚\")\n",
    "    # åªæœ‰åœ¨å¤šæ–¼ä¸€å¼µå½±åƒæ™‚æ‰ä½¿ç”¨ mosaic\n",
    "    flood_image = s1_collection.mosaic().clip(roi_choose1)\n",
    "\n",
    "band = 'VV'\n",
    "if flood_image is not None:\n",
    "    flood_image = flood_image.select(band)\n",
    "    Map.addLayer(flood_image, {'min':-25,'max':5}, 'Flood Image')\n",
    "    print(\"Flood Image å·²æˆåŠŸåŠ è¼‰ã€‚\")\n",
    "else:\n",
    "    print(\"éŒ¯èª¤ï¼šç„¡æ³•ç”Ÿæˆ flood_imageï¼Œè«‹æª¢æŸ¥æ—¥æœŸå’Œç ”ç©¶å€åŸŸã€‚\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce091bac",
   "metadata": {},
   "source": [
    "## å¯¼å‡ºç ”ç©¶åŒºå’Œå½±åƒåˆ°è°·æ­Œäº‘ç›˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 ä½¿ç”¨Google Earth EngineåŸç”Ÿå¯¼å‡ºï¼ˆä¿®å¤æ–‡ä»¶å¤¹é—®é¢˜ï¼‰\n",
    "print(\"\\n=== ä½¿ç”¨GEEåŸç”Ÿå¯¼å‡ºæ•°æ®åˆ°Google Drive ===\")\n",
    "\n",
    "# æ£€æŸ¥å¿…è¦çš„æ•°æ®æ˜¯å¦å­˜åœ¨\n",
    "export_ready = True\n",
    "if roi_choose1 is None:\n",
    "    print(\"âŒ é”™è¯¯ï¼šroi_choose1 æœªå®šä¹‰\")\n",
    "    export_ready = False\n",
    "if flood_image is None:\n",
    "    print(\"âŒ é”™è¯¯ï¼šflood_image æœªå®šä¹‰\")\n",
    "    export_ready = False\n",
    "\n",
    "if export_ready:\n",
    "    folder = 'SCI_Valencia'  # Google Driveæ–‡ä»¶å¤¹å\n",
    "    \n",
    "    # åˆ›å»ºä¸€ä¸ªç»Ÿä¸€çš„å¯¼å‡ºä»»åŠ¡åˆ—è¡¨ï¼Œç¡®ä¿ä½¿ç”¨ç›¸åŒçš„æ–‡ä»¶å¤¹\n",
    "    export_tasks = []\n",
    "    \n",
    "    # ä»»åŠ¡1ï¼šå¯¼å‡ºROIçŸ¢é‡ï¼ˆä»¥Shapefileæ ¼å¼ï¼‰\n",
    "    print(\"\\næ­¥éª¤1: å¯¼å‡ºROIçŸ¢é‡...\")\n",
    "    try:\n",
    "        # å°†ROIè½¬æ¢ä¸ºFeatureCollectionï¼Œå¹¶æ·»åŠ å±æ€§\n",
    "        roi_feature = ee.Feature(roi_choose1, {'name': 'study_area', 'date': targdate})\n",
    "        roi_fc = ee.FeatureCollection([roi_feature])\n",
    "        \n",
    "        # ä½¿ç”¨åŸç”Ÿee.batch.Exportå¯¼å‡ºçŸ¢é‡\n",
    "        vector_task = ee.batch.Export.table.toDrive(\n",
    "            collection=roi_fc,\n",
    "            description=f'ROI_Vector_{targdate}',\n",
    "            folder=folder,\n",
    "            fileNamePrefix=f'ROI_Vector_{targdate}',\n",
    "            fileFormat='SHP'\n",
    "        )\n",
    "        export_tasks.append(('ROI Vector', vector_task))\n",
    "        print(f\"  âœ“ ROIçŸ¢é‡å¯¼å‡ºä»»åŠ¡å·²å‡†å¤‡ï¼šROI_Vector_{targdate}.shp\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ROIçŸ¢é‡å¯¼å‡ºå‡†å¤‡å¤±è´¥ï¼š{str(e)}\")\n",
    "    \n",
    "    # ä»»åŠ¡2ï¼šå¯¼å‡ºè£å‰ªåçš„SARå½±åƒï¼ˆç ”ç©¶åŒºåŸŸï¼‰\n",
    "    print(\"\\næ­¥éª¤2: å¯¼å‡ºè£å‰ªåçš„SARå½±åƒ...\")\n",
    "    try:\n",
    "        # ç¡®ä¿å½±åƒæ˜¯å•æ³¢æ®µä¸”å·²è£å‰ª\n",
    "        clipped_image = flood_image.select('VV').clip(roi_choose1)\n",
    "        \n",
    "        # ä½¿ç”¨åŸç”Ÿee.batch.Exportå¯¼å‡ºå½±åƒ\n",
    "        clipped_task = ee.batch.Export.image.toDrive(\n",
    "            image=clipped_image,\n",
    "            description=f'SAR_Clipped_{targdate}',\n",
    "            folder=folder,\n",
    "            fileNamePrefix=f'SAR_Clipped_{targdate}',\n",
    "            scale=10,\n",
    "            region=roi_choose1,\n",
    "            crs='EPSG:4326',\n",
    "            maxPixels=1e13\n",
    "        )\n",
    "        export_tasks.append(('SAR Clipped', clipped_task))\n",
    "        print(f\"  âœ“ è£å‰ªSARå½±åƒå¯¼å‡ºä»»åŠ¡å·²å‡†å¤‡ï¼šSAR_Clipped_{targdate}.tif\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ è£å‰ªå½±åƒå¯¼å‡ºå‡†å¤‡å¤±è´¥ï¼š{str(e)}\")\n",
    "    \n",
    "    # ä»»åŠ¡3ï¼šå¯¼å‡ºå®Œæ•´æ™¯SARå½±åƒ\n",
    "    print(\"\\næ­¥éª¤3: å¯¼å‡ºå®Œæ•´æ™¯SARå½±åƒ...\")\n",
    "    try:\n",
    "        # è·å–å®Œæ•´çš„åŸå§‹å½±åƒï¼ˆä¸è£å‰ªï¼‰\n",
    "        if s1_size == 1:\n",
    "            full_scene_image = ee.Image(s1_collection.first()).select('VV')\n",
    "        else:\n",
    "            full_scene_image = s1_collection.mosaic().select('VV')\n",
    "        \n",
    "        # ä½¿ç”¨åŸç”Ÿee.batch.Exportå¯¼å‡ºå®Œæ•´æ™¯å½±åƒ\n",
    "        fullscene_task = ee.batch.Export.image.toDrive(\n",
    "            image=full_scene_image,\n",
    "            description=f'SAR_FullScene_{targdate}',\n",
    "            folder=folder,\n",
    "            fileNamePrefix=f'SAR_FullScene_{targdate}',\n",
    "            scale=10,\n",
    "            region=full_scene_image.geometry(),\n",
    "            crs='EPSG:4326',\n",
    "            maxPixels=1e13\n",
    "        )\n",
    "        export_tasks.append(('SAR Full Scene', fullscene_task))\n",
    "        print(f\"  âœ“ å®Œæ•´æ™¯SARå½±åƒå¯¼å‡ºä»»åŠ¡å·²å‡†å¤‡ï¼šSAR_FullScene_{targdate}.tif\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ å®Œæ•´æ™¯å½±åƒå¯¼å‡ºå‡†å¤‡å¤±è´¥ï¼š{str(e)}\")\n",
    "    \n",
    "    # æ‰¹é‡å¯åŠ¨æ‰€æœ‰å¯¼å‡ºä»»åŠ¡ï¼ˆç¡®ä¿ä½¿ç”¨ç›¸åŒæ–‡ä»¶å¤¹ï¼‰\n",
    "    print(f\"\\nğŸ“¤ æ‰¹é‡å¯åŠ¨å¯¼å‡ºä»»åŠ¡...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    successful_exports = 0\n",
    "    for task_name, task in export_tasks:\n",
    "        try:\n",
    "            task.start()\n",
    "            print(f\"  âœ“ {task_name} å¯¼å‡ºä»»åŠ¡å·²å¯åŠ¨\")\n",
    "            successful_exports += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {task_name} å¯¼å‡ºä»»åŠ¡å¯åŠ¨å¤±è´¥ï¼š{str(e)}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå¯¼å‡ºçŠ¶æ€ä¿¡æ¯\n",
    "    print(f\"\\nğŸ“Š å¯¼å‡ºä»»åŠ¡æ€»ç»“:\")\n",
    "    print(f\"  âœ“ ç›®æ ‡æ—¥æœŸ: {targdate}\")\n",
    "    print(f\"  âœ“ å¯¼å‡ºæ–‡ä»¶å¤¹: {folder} (Google Drive)\")\n",
    "    print(f\"  âœ“ å½±åƒåˆ†è¾¨ç‡: 10ç±³\")\n",
    "    print(f\"  âœ“ åæ ‡ç³»ç»Ÿ: EPSG:4326\")\n",
    "    print(f\"  âœ“ æˆåŠŸå¯åŠ¨ä»»åŠ¡æ•°: {successful_exports}/{len(export_tasks)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ æ–‡ä»¶æ¸…å•:\")\n",
    "    print(f\"  1. ROI_Vector_{targdate}.shp - ç ”ç©¶åŒºåŸŸçŸ¢é‡ï¼ˆShapefileæ ¼å¼ï¼‰\")\n",
    "    print(f\"  2. SAR_Clipped_{targdate}.tif - è£å‰ªåçš„SARå½±åƒ\")\n",
    "    print(f\"  3. SAR_FullScene_{targdate}.tif - å®Œæ•´æ™¯SARå½±åƒ\")\n",
    "    \n",
    "    print(f\"\\nâ³ ä»»åŠ¡çŠ¶æ€è¯´æ˜:\")\n",
    "    print(f\"  â€¢ æ‰€æœ‰å¯¼å‡ºä»»åŠ¡å·²æäº¤åˆ°Google Earth Engine\")\n",
    "    print(f\"  â€¢ æ‰€æœ‰æ–‡ä»¶å°†ä¿å­˜åˆ°åŒä¸€ä¸ª '{folder}' æ–‡ä»¶å¤¹ä¸­\")\n",
    "    print(f\"  â€¢ è¯·è®¿é—® https://code.earthengine.google.com æŸ¥çœ‹Tasksæ ‡ç­¾é¡µ\")\n",
    "    print(f\"  â€¢ ä»»åŠ¡å®Œæˆåï¼Œæ–‡ä»¶å°†å‡ºç°åœ¨æ‚¨çš„Google Driveä¸­\")\n",
    "    print(f\"  â€¢ å¤§æ–‡ä»¶å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶å®Œæˆ\")\n",
    "    \n",
    "    # ä¿å­˜å¯¼å‡ºä»»åŠ¡ä¿¡æ¯ä¾›åç»­æŸ¥è¯¢\n",
    "    global export_task_info\n",
    "    export_task_info = {\n",
    "        'folder': folder,\n",
    "        'target_date': targdate,\n",
    "        'tasks': [(name, task.id if hasattr(task, 'id') else 'Unknown') for name, task in export_tasks],\n",
    "        'export_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ æç¤º:\")\n",
    "    print(f\"  â€¢ å¦‚æœä»ç„¶å‡ºç°å¤šä¸ªåŒåæ–‡ä»¶å¤¹ï¼Œè¿™æ˜¯GEEçš„æ­£å¸¸è¡Œä¸º\")\n",
    "    print(f\"  â€¢ æ‚¨å¯ä»¥åœ¨Google Driveä¸­æ‰‹åŠ¨åˆå¹¶æ–‡ä»¶åˆ°ä¸€ä¸ªæ–‡ä»¶å¤¹\")\n",
    "    print(f\"  â€¢ æˆ–è€…ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆåï¼ŒæŒ‰æ–‡ä»¶åå‰ç¼€æ•´ç†æ–‡ä»¶\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ ç¼ºå°‘å¿…è¦æ•°æ®ï¼Œæ— æ³•æ‰§è¡Œå¯¼å‡ºæ“ä½œ\")\n",
    "    print(\"   è¯·ç¡®ä¿å·²æˆåŠŸè¿è¡Œå‰é¢çš„ä»£ç å¹¶å®šä¹‰äº†roi_choose1å’Œflood_image\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ç¬¬4ç« æ•°æ®å¯¼å‡ºæ¨¡å—æ‰§è¡Œå®Œæ¯•\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb519c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30bfb1e5",
   "metadata": {},
   "source": [
    "# 5.è‡ªåŠ¨é˜ˆå€¼åˆ†å‰²\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#äºŒå€¼åŒ–+å¼€è¿ç®—\n",
    "\n",
    "globalThreshold = ee.Number(-10.5)\n",
    "globalWater = flood_image.lt(globalThreshold)\n",
    "corrosion_kernel = ee.Kernel.circle(radius = 5)\n",
    "dilation_kernel = ee.Kernel.circle(radius = 3)\n",
    "eroded_Dark_targets = globalWater.focal_min(kernel = corrosion_kernel, iterations = 1)\n",
    "#.focal_max(kernel = dilation_kernel, iterations = 1)\n",
    "Map.addLayer(eroded_Dark_targets.selfMask(), {'palette':['red']}, 'Eroded Water')\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c3b96",
   "metadata": {},
   "source": [
    "# 6.è¾¹ç¼˜æ£€æµ‹+ç¼“å†²åŒº+å¤–åŒ…çŸ©å½¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa63afe7",
   "metadata": {},
   "source": [
    "## 6.1 è¾¹ç¼˜æ£€æµ‹+ç¼“å†²åŒº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a509a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ–¹æ³•éœ€è¦çš„å‚æ•°\n",
    "connectedPixels = 100 # é•¿åº¦è®¡ç®—è¿æ¥åƒç´ æ•°\n",
    "edgeLength = 50 #æ°´è¾¹ç¼˜çš„é•¿åº¦\n",
    "edgeBuffer = 50 #è¾¹ç¼˜çš„ç¼“å†²åŒº(å•ä½m)\n",
    "cannyThreshold = 1 #cannyè¾¹ç¼˜æ£€æµ‹çš„é˜ˆå€¼\n",
    "cannySigma = 1 #Canny è¾¹ç¼˜æ£€æµ‹ä¸­é«˜æ–¯æ»¤æ³¢å™¨çš„ Sigma å€¼\n",
    "cannyLt = 0.05 #cannyè¾¹ç¼˜æ£€æµ‹çš„æ›´é™åˆ¶çš„é˜ˆå€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cannyè¾¹ç¼˜æ£€æµ‹\n",
    "canny = ee.Algorithms.CannyEdgeDetector(\n",
    "    image=eroded_Dark_targets,\n",
    "    threshold=cannyThreshold,\n",
    "    sigma=cannySigma\n",
    ")\n",
    "\n",
    "#è·å–è¾¹ç¼˜ï¼ˆæ’é™¤å¼ºè¾¹ç¼˜ åªè¦å¼±è¾¹ç¼˜ å› å¼ºè¾¹ç¼˜ä¼šæ˜¯å™ªå£° å³è¾¹ç¼˜å¼ºåº¦å°äºé˜ˆå€¼ï¼‰\n",
    "#connectedPixelCount()å‡½æ•°è®¡ç®—æ¯ä¸ªåƒç´ çš„ç›¸é‚»ç›¸è¿åƒç´ çš„æ•°é‡ï¼Œå³è¾¹ç¼˜çš„é•¿åº¦\n",
    "connected = canny.updateMask(canny).lt(cannyLt).connectedPixelCount(connectedPixels,True)\n",
    "\n",
    "#æ–°çš„åƒç´ æ•°é‡å›¾åƒä¸­ï¼Œæ¯ä¸ªåƒç´ å€¼è¡¨ç¤ºåŸå§‹å›¾åƒä¸­ï¼Œä¸å…¶ç›¸è¿çš„ã€ä¸”è¾¹ç¼˜å¼ºåº¦å°äºcannyLtçš„åƒç´ çš„æ•°é‡\n",
    "#è®©çŸ­è¾¹ç¼˜è§†ä¸ºå™ªå£°\n",
    "edges = connected.gte(edgeLength)\n",
    "\n",
    "#åŸºäºè¾¹ç¼˜åˆ›å»ºç¼“å†²åŒºï¼Œå¹¶å°†ç¼“å†²åŒºå†…çš„åƒç´ è®¾ä¸º1\n",
    "#fastDistanceTransform():å¿«é€Ÿè·ç¦»å˜æ¢å‡½æ•°ï¼šè®¡ç®—æ¯ä¸ªåƒç´ åˆ°æœ€è¿‘çš„éé›¶åƒç´ ï¼ˆè¾¹ç¼˜åƒç´ ï¼‰çš„è·ç¦»\n",
    "bufferEdges = edges.fastDistanceTransform().lte(edgeBuffer)\n",
    "#å°†ç¼“å†²åŒºå¤–éƒ¨åˆ†æ©è†œ\n",
    "edgeImage = flood_image.select(band).updateMask(bufferEdges)\n",
    "\n",
    "edgeVis = {'palette' : 'yellow' , 'opacity' : 0.5}\n",
    "\n",
    "# å¯è§†åŒ–è¾¹ç¼˜å’Œç¼“å†²åŒº\n",
    "Map.addLayer(edges.selfMask(), {'palette': 'blue'}, 'Detected Edges')\n",
    "Map.addLayer(bufferEdges.selfMask(), edgeVis, 'Edge Buffer Zones')\n",
    "Map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31636890",
   "metadata": {},
   "source": [
    "# 6.2 è¾¹ç¼˜ç¼“å†²åŒºç”Ÿæˆå¤–åŒ…çŸ©å½¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec713eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"æ­£åœ¨å°†æ …æ ¼æ–‘å—è½¬ä¸ºçŸ¢é‡\")\n",
    "bufferVectors = bufferEdges.selfMask().reduceToVectors(\n",
    "    geometry = roi_choose1,\n",
    "    scale = 10,\n",
    "    geometryType = 'polygon',\n",
    "    eightConnected = True,\n",
    "    labelProperty = 'labels',\n",
    "    maxPixels = 1e10\n",
    ")\n",
    "bufferVectors = bufferVectors\n",
    "#.filter(ee.Filter.gt('count',1000/(30*30)))\n",
    "\n",
    "print('æ­£åœ¨ä¸ºæ¯ä¸ªçŸ¢é‡è®¡ç®—å¤–åŒ…çŸ©å½¢')\n",
    "def getBoundingBox(feature):\n",
    "   # feature.geometry() è·å–å½“å‰è¦ç´ çš„å‡ ä½•å½¢çŠ¶\n",
    "  # .bounds() è®¡ç®—è¯¥å‡ ä½•å½¢çŠ¶çš„æœ€å°å¤–åŒ…çŸ©å½¢\n",
    "  # ee.Feature() ç”¨æ–°çš„çŸ©å½¢å‡ ä½•ä½“åˆ›å»ºä¸€ä¸ªæ–°çš„è¦ç´ ï¼Œå¹¶ä¿ç•™åŸå§‹å±æ€§\n",
    "  return ee.Feature(feature.geometry().bounds()) # Removed feature.properties\n",
    "\n",
    "#å°†ä¸Šè¿°å‡½æ•°åº”ç”¨åˆ°é›†åˆä¸­çš„æ¯ä¸€ä¸ªè¦ç´ \n",
    "boundingBoxes = bufferVectors.map(getBoundingBox)\n",
    "\n",
    "# ---\n",
    "# æ­¥éª¤ 4: å¯è§†åŒ–å¤–åŒ…çŸ©å½¢\n",
    "# ---\n",
    "# ä¸ºäº†åªæ˜¾ç¤ºè¾¹æ¡†è€Œä¸æ˜¯å®å¿ƒçŸ©å½¢ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç©ºçš„å›¾åƒï¼Œç„¶åå°†çŸ©å½¢çš„è¾¹æ¡†â€œç”»â€ä¸Šå»ã€‚\n",
    "# è¿™æ¯”ç›´æ¥æ·»åŠ  FeatureCollection å¹¶è®¾ç½®æ ·å¼æ›´ç¨³å¥ã€‚\n",
    "empty = ee.Image().byte() # åˆ›å»ºä¸€ä¸ªç©ºçš„8ä½å›¾åƒä½œä¸ºç”»å¸ƒ\n",
    "\n",
    "# ä½¿ç”¨ .paint() å‡½æ•°ç»˜åˆ¶è¾¹æ¡†\n",
    "outline = empty.paint(\n",
    "    featureCollection=boundingBoxes, # è¦ç»˜åˆ¶çš„è¦ç´ é›†åˆ\n",
    "    color=1, # è¾¹æ¡†çš„é¢œè‰²ï¼Œè¿™é‡Œè®¾ä¸º1ï¼Œä»¥ä¾¿ä½¿ç”¨ç»Ÿä¸€é¢œè‰²\n",
    "    width=2  # è¾¹æ¡†çš„å®½åº¦ï¼ˆåƒç´ ï¼‰\n",
    ")\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªé²œè‰³çš„é¢œè‰²æ–¹æ¡ˆæ¥æ˜¾ç¤ºå¤–åŒ…çŸ©å½¢\n",
    "bboxVis = {'palette': 'FF00FF'} # ä½¿ç”¨äº®ç´«è‰² (Magenta)\n",
    "\n",
    "print(\"å°†å¤–åŒ…çŸ©å½¢æ·»åŠ åˆ°åœ°å›¾...\")\n",
    "Map.addLayer(outline, bboxVis, 'Detected Bounding Boxes')\n",
    "\n",
    "# å°†åœ°å›¾ä¸­å¿ƒè®¾ç½®åˆ°æ‚¨çš„ç ”ç©¶åŒº\n",
    "Map.centerObject(roi_choose1, 11)\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df285c7b",
   "metadata": {},
   "source": [
    "# 7.å¤–åŒ…çŸ©å½¢ç­›é€‰åŠæå–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205832e",
   "metadata": {},
   "source": [
    "## 7.1 çŸ©å½¢ç­›é€‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#ç­›é€‰ç­–ç•¥ï¼šåŸºäºå‰–é¢çº¿â€œç¦»ç¾¤ç‚¹ç™¾åˆ†æ¯”â€çš„ç­›é€‰æ³•\n",
    "def analyze_and_filter_bbox_by_outlier_percentage(feature):\n",
    "    try:\n",
    "        error_margin = ee.ErrorMargin(1) # è®¾ç½®è¯¯å·®èŒƒå›´ä¸º1ç±³\n",
    "        bbox_geom = feature.geometry() # è·å–å¤–åŒ…çŸ©å½¢çš„å‡ ä½•å½¢çŠ¶\n",
    "        center = bbox_geom.centroid(error_margin) # è®¡ç®—å¤–åŒ…çŸ©å½¢çš„ä¸­å¿ƒç‚¹\n",
    "        coords = ee.List(bbox_geom.coordinates().get(0)) # è·å–å¤–åŒ…çŸ©å½¢çš„åæ ‡åˆ—è¡¨\n",
    "        \n",
    "        #è·å–çŸ©å½¢çš„ä¸‰ä¸ªè§’ç‚¹\n",
    "        p1, p2, p3 = ee.List(coords.get(0)), ee.List(coords.get(1)), ee.List(coords.get(2))\n",
    "        \n",
    "        # å®¢æˆ·ç«¯å‡ ä½•è®¡ç®—\n",
    "        p1_coords, p2_coords, p3_coords = np.array(p1.getInfo()), np.array(p2.getInfo()), np.array(p3.getInfo())\n",
    "        center_coords= np.array(center.coordinates().getInfo())\n",
    "        vector_12, vector_23 = p2_coords - p1_coords, p3_coords - p2_coords  # è®¡ç®—ä¸¤ä¸ªè¾¹çš„å‘é‡\n",
    "        \n",
    "        # ---æ–°å¢:é•¿å®½æ¯”ç­›é€‰---\n",
    "        side1_length = np.linalg.norm(vector_12)\n",
    "        side2_length = np.linalg.norm(vector_23)\n",
    "        if side1_length < side2_length:\n",
    "            side1_length, side2_length = side2_length, side1_length # ç¡®ä¿ side1_length æ˜¯è¾ƒé•¿çš„è¾¹\n",
    "        #è®¡ç®—é•¿å®½æ¯”å¹¶æ£€æŸ¥\n",
    "        aspect_ratio = side1_length / side2_length if side2_length != 0 else float('inf')\n",
    "        if aspect_ratio > MAX_ASPECT_RATIO:\n",
    "            return feature.set({\n",
    "                'is_valid':ee.Number(0),\n",
    "                'outelier_percentage':-1,\n",
    "                'reason':f'AspectRatioTooHigh ({aspect_ratio:.1f}:1)'\n",
    "            })\n",
    "        # --ç»“æŸ--\n",
    "        main_axis_vector = vector_12 if np.linalg.norm(vector_12) < np.linalg.norm(vector_23) else vector_23  # é€‰æ‹©è¾ƒçŸ­çš„è¾¹ä½œä¸ºä¸»è½´\n",
    "        start_point = center_coords - main_axis_vector * 1.5\n",
    "        end_point = center_coords + main_axis_vector * 1.5\n",
    "        \n",
    "        # åœ¨GEEä¸­åˆ›å»ºå‰–é¢çº¿\n",
    "        long_line = ee.Geometry.LineString([start_point.tolist(), end_point.tolist()])\n",
    "        profile_line = long_line.intersection(bbox_geom, error_margin)\n",
    "        \n",
    "        # ä½¿ç”¨reduceRegionè¿›è¡Œæ²¿çº¿é‡‡æ ·\n",
    "        sampled_dict = flood_image.select('VV').reduceRegion(\n",
    "            reducer=ee.Reducer.toList(),\n",
    "            geometry=profile_line,\n",
    "            scale=10,\n",
    "            maxPixels=1e10\n",
    "        )\n",
    "        \n",
    "        sar_values = sampled_dict.get('VV').getInfo()\n",
    "        # å¦‚æœé‡‡æ ·ç‚¹æ•°ä¸è¶³ï¼Œç›´æ¥è¿”å›æ— æ•ˆ\n",
    "        if not sar_values or len(sar_values) < 3:\n",
    "            return feature.set({\n",
    "                'is_valid': ee.Number(0),\n",
    "                'outlier_percentage': -1,\n",
    "                'profile_line': profile_line,\n",
    "                'reason': 'NotEnoughPoints'\n",
    "            })\n",
    "\n",
    "        # å®¢æˆ·ç«¯è®¡ç®—\n",
    "        sar_array = np.array(sar_values)\n",
    "        outlier_count = np.sum(sar_array < OUTLIER_SAR_THRESHOLD)\n",
    "        total_count = len(sar_array)\n",
    "        outlier_percentage = outlier_count / total_count\n",
    "\n",
    "        is_valid_python = outlier_percentage >= MIN_OUTLIER_PERCENTAGE and outlier_percentage <= MAX_OUTLIER_PERCENTAGE\n",
    "\n",
    "        return feature.set({\n",
    "            'is_valid': ee.Number(1) if is_valid_python else ee.Number(0), #ä¸èƒ½ç›´æ¥è®¾ä¸ºTrue or Falseå¸ƒå°”å‹ï¼Œå› ä¸ºä¸geeæ•°æ®æ ¼å¼ä¸å…¼å®¹\n",
    "            'outlier_percentage': outlier_percentage,\n",
    "            'profile_line': profile_line,\n",
    "            'reason': 'Passed' if is_valid_python else 'LowPercentage'\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return feature.set({\n",
    "            'is_valid': ee.Number(0),\n",
    "            'outlier_percentage': -1,\n",
    "            'reason': f'Error: {str(e)}'\n",
    "        })\n",
    "\n",
    "# ==============================================================================\n",
    "# å‚æ•°å®šä¹‰\n",
    "# ==============================================================================\n",
    "OUTLIER_SAR_THRESHOLD = -13.5\n",
    "MIN_OUTLIER_PERCENTAGE = 0.1\n",
    "MAX_OUTLIER_PERCENTAGE = 0.9\n",
    "MAX_ASPECT_RATIO = 5.0\n",
    "# ==============================================================================\n",
    "# æ ¸å¿ƒè°ƒè¯•å¾ªç¯ (æœ€ç»ˆä¿®å¤ç‰ˆ)\n",
    "# ==============================================================================\n",
    "bboxes_list = boundingBoxes.toList(boundingBoxes.size())\n",
    "processed_features_list = []\n",
    "# [ä¿®å¤] æˆ‘ä»¬ä¸å†éœ€è¦ all_profile_lines åˆ—è¡¨ï¼Œç›´æ¥ä»æœ€ç»ˆé›†åˆä¸­æå–\n",
    "\n",
    "print(\"\\n--- [æœ€ç»ˆä¿®å¤ç‰ˆ v2] é€ä¸ªæ£€æŸ¥æ¯ä¸ªçŸ©å½¢çš„å¤„ç†è¿‡ç¨‹ ---\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Rect ID':<10} | {'Outlier %':<15} | {'Status':<15} | {'Reason/Details'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i in range(bboxes_list.size().getInfo()):\n",
    "    server_feature = ee.Feature(bboxes_list.get(i))\n",
    "    client_processed_feature = analyze_and_filter_bbox_by_outlier_percentage(server_feature)\n",
    "\n",
    "    # [ä¿æŒä¸å˜] æ‰“å°ä¸»ä¿¡æ¯\n",
    "    properties = client_processed_feature.getInfo()['properties']\n",
    "    is_valid_flag = properties.get('is_valid', 0) == 1\n",
    "    percentage = properties.get('outlier_percentage', -1)\n",
    "    status = \"PASSED\" if is_valid_flag else \"FAILED\"\n",
    "    print(f\"{i:<10} | {f'{percentage:.2%}':<15} | {status:<15} | {properties.get('reason', 'N/A')}\")\n",
    "\n",
    "    # [ä¿æŒä¸å˜] æ‰“å°å¤±è´¥çš„è¯¦ç»†ä¿¡æ¯\n",
    "    if not is_valid_flag and percentage != -1:\n",
    "        profile_line = client_processed_feature.get('profile_line')\n",
    "        if profile_line is not None:\n",
    "            profile_line_geom = ee.Geometry(profile_line)\n",
    "            sampled_dict_debug = flood_image.select('VV').reduceRegion(\n",
    "                reducer=ee.Reducer.toList(),\n",
    "                geometry=profile_line_geom,\n",
    "                scale=10\n",
    "            )\n",
    "            values_debug_list = ee.List(sampled_dict_debug.get('VV')).getInfo()\n",
    "            values_str = [f\"{v:.2f}\" for v in values_debug_list] if values_debug_list else [\"No values sampled\"]\n",
    "            print(f\"{'':<10} | {'':<15} | {'':<15} | RZ Values: [{', '.join(values_str)}]\")\n",
    "\n",
    "    processed_features_list.append(client_processed_feature)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==============================================================================\n",
    "# åç»­å¤„ç†å’Œå¯è§†åŒ– (æœ€ç»ˆä¿®å¤ç‰ˆ)\n",
    "# ==============================================================================\n",
    "analyzed_bboxes = ee.FeatureCollection(processed_features_list)\n",
    "filtered_boundingBoxes = analyzed_bboxes.filter(ee.Filter.eq('is_valid', 1))\n",
    "\n",
    "print(f\"\\næœ€ç»ˆç»“æœï¼š\")\n",
    "print(f\"ç­›é€‰å‰çŸ©å½¢æ€»æ•°: {analyzed_bboxes.size().getInfo()}\")\n",
    "print(f\"ç­›é€‰åï¼ˆç¦»ç¾¤ç‚¹ç™¾åˆ†æ¯” >= {MIN_OUTLIER_PERCENTAGE:.0%}ï¼‰çš„çŸ©å½¢æ•°é‡: {filtered_boundingBoxes.size().getInfo()}\")\n",
    "\n",
    "# --- å¯è§†åŒ– ---\n",
    "\n",
    "# [æ ¸å¿ƒä¿®å¤] å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå°†Featureçš„å‡ ä½•ä½“æ›¿æ¢ä¸ºå…¶'profile_line'å±æ€§\n",
    "def extract_profile_as_feature(feature):\n",
    "    # ä»è¾“å…¥ feature ä¸­è·å– profile_line å‡ ä½•ä½“\n",
    "    profile_geom = ee.Geometry(feature.get('profile_line'))\n",
    "    # è¿”å›ä¸€ä¸ªæ–°çš„ Featureï¼Œå®ƒçš„å‡ ä½•ä½“æ˜¯ profile_lineï¼Œå¹¶ä¸”ä¸åŒ…å«ä»»ä½•å±æ€§\n",
    "    # è¿™æ ·å¯ä»¥é¿å…å±æ€§ç»§æ‰¿å¸¦æ¥çš„é—®é¢˜\n",
    "    return ee.Feature(profile_geom)\n",
    "\n",
    "# -- å¯è§†åŒ–æ‰€æœ‰å‰–é¢çº¿ (ç”¨äºè°ƒè¯•) --\n",
    "# ä½¿ç”¨ .map() åº”ç”¨ä¸Šé¢çš„å‡½æ•°ï¼Œç¡®ä¿æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª FeatureCollection of LineStrings\n",
    "all_profiles_fc = analyzed_bboxes.map(extract_profile_as_feature)\n",
    "Map.addLayer(ee.Image().byte().paint(all_profiles_fc, 0, 1), {'palette': 'FF0000'}, 'All Profile Lines (DEBUG)') # çº¢è‰²\n",
    "\n",
    "# -- å¯è§†åŒ–é€šè¿‡ç­›é€‰çš„çŸ©å½¢å’Œå‰–é¢çº¿ --\n",
    "if filtered_boundingBoxes.size().getInfo() > 0:\n",
    "    # å¯è§†åŒ–çŸ©å½¢è¾¹æ¡† (è¿™éƒ¨åˆ†æ²¡é—®é¢˜)\n",
    "    filtered_outline = ee.Image().byte().paint(filtered_boundingBoxes, 0, 2)\n",
    "    Map.addLayer(filtered_outline, {'palette': '00008B'}, 'Filtered BBoxes (PASSED)') # ç»¿è‰²\n",
    "\n",
    "    # [æ ¸å¿ƒä¿®å¤] ä½¿ç”¨åŒæ ·çš„æ–¹æ³•æå–é€šè¿‡ç­›é€‰çš„å‰–é¢çº¿\n",
    "    valid_profile_lines_fc = filtered_boundingBoxes.map(extract_profile_as_feature)\n",
    "    valid_profile_lines = ee.Image().byte().paint(valid_profile_lines_fc, 0, 2)\n",
    "    Map.addLayer(valid_profile_lines, {'palette': '00FFFF'}, 'Valid Profile Lines (PASSED)') # é’è‰²\n",
    "else:\n",
    "    print(\"\\næ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„çŸ©å½¢ã€‚\")\n",
    "\n",
    "Map.centerObject(roi_choose1, 11)\n",
    "Map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e88a3c",
   "metadata": {},
   "source": [
    "## 7.2 å‡å€¼å‰–é¢çº¿æå–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5746371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "import pandas as pd\n",
    "\n",
    "def create_mean_profile_line(feature):\n",
    "    \"\"\"\n",
    "    ç‚ºæ¯å€‹é€šéç¯©é¸çš„çŸ©å½¢å‰µå»ºå‡å€¼å‰–é¢ç·š\n",
    "    Args:\n",
    "        feature: ee.Featureï¼ŒåŒ…å«å¤–åŒ…çŸ©å½¢å’Œç›¸é—œå±¬æ€§\n",
    "    Returns:\n",
    "        dict,ä¸€ä¸ªå­—å…¸ï¼Œå„ä¸ªå¯¹åº”çš„å±æ€§åŒ…å«å‡å€¼å‰–é¢ç·šçš„å¹¾ä½•å½¢ç‹€å’Œç›¸é—œå±¬æ€§\n",
    "    \"\"\"\n",
    "    try:\n",
    "        error_margin = ee.ErrorMargin(1)\n",
    "        bbox_geom = feature.geometry()\n",
    "        center = bbox_geom.centroid(error_margin)\n",
    "        coords = ee.List(bbox_geom.coordinates().get(0))\n",
    "        \n",
    "        # ç²å–çŸ©å½¢çš„ä¸‰å€‹è§’é»\n",
    "        p1, p2, p3 = ee.List(coords.get(0)), ee.List(coords.get(1)), ee.List(coords.get(2))\n",
    "        \n",
    "        # å®¢æˆ¶ç«¯å¹¾ä½•è¨ˆç®—\n",
    "        p1_coords, p2_coords, p3_coords = np.array(p1.getInfo()), np.array(p2.getInfo()), np.array(p3.getInfo())\n",
    "        center_coords = np.array(center.coordinates().getInfo())\n",
    "        vector_12, vector_23 = p2_coords - p1_coords, p3_coords - p2_coords\n",
    "        \n",
    "        # é¸æ“‡è¼ƒçŸ­çš„é‚Šä½œç‚ºä¸»è»¸ï¼ˆæ²¿çŸ­é‚Šæ–¹å‘ï¼‰\n",
    "        if np.linalg.norm(vector_12) < np.linalg.norm(vector_23):\n",
    "            main_axis_vector = vector_12\n",
    "            perpendicular_vector = vector_23\n",
    "        else:\n",
    "            main_axis_vector = vector_23\n",
    "            perpendicular_vector = vector_12\n",
    "            \n",
    "        # --æ­£ç¡®çš„è·ç¦»è®¡ç®—---\n",
    "        # è®¡ç®—çŸ©å½¢åœ¨åœ°é¢ä¸Šçš„å®é™…å°ºå¯¸\n",
    "        bbox_bounds = bbox_geom.bounds().getInfo()\n",
    "        # [0][0]æ˜¯å·¦ä¸‹è§’ç‚¹ï¼Œ ã€0][2]æ˜¯å³ä¸Šè§’ç‚¹ \n",
    "        west, south, east, north = bbox_bounds['coordinates'][0][0][0], bbox_bounds['coordinates'][0][0][1], \\\n",
    "                                   bbox_bounds['coordinates'][0][2][0], bbox_bounds['coordinates'][0][2][1]\n",
    "        \n",
    "        # è¿›è¡Œç®€åŒ–çš„ç»çº¬åº¦è½¬ç±³è®¡ç®—\n",
    "        # åœ¨ä¸­çº¬åº¦åœ°åŒºï¼Œ1Â°ç»åº¦=111320*cos(çº¬åº¦)ï¼Œ1Â°çº¬åº¦=111320ç±³\n",
    "        lat_center = (south + north) / 2\n",
    "        meters_per_degree_lon = 111320 * np.cos(np.radians(lat_center)) # ç»åº¦æ¯åº¦å¯¹åº”çš„ç±³æ•°\n",
    "        meters_per_degree_lat = 111320 # çº¬åº¦æ¯åº¦å¯¹åº”çš„ç±³æ•°\n",
    "        \n",
    "        #è®¡ç®—çŸ©å½¢çš„å®é™…è·ç¦»ï¼ˆç±³ï¼‰\n",
    "        width_meters = abs(east-west) * meters_per_degree_lon\n",
    "        height_meters = abs(north-south) * meters_per_degree_lat\n",
    "        short_edge_length_meters = min(width_meters, height_meters)\n",
    "        \n",
    "        \n",
    "        # è¨ˆç®—L = l/4ï¼ˆlç‚ºçŸ­é‚Šé•·åº¦ï¼‰\n",
    "        L_meters = short_edge_length_meters / 8\n",
    "        buffer_width_meters = 2 * L_meters  # ç·©è¡å€å¯¬åº¦ç‚º2L\n",
    "        \n",
    "        # å°‡ç·©è¡å€å¯¬åº¦è½‰æ›ç‚ºåº¦æ•¸å–®ä½ï¼ˆç”¨æ–¼GEEæ“ä½œï¼‰\n",
    "        if width_meters < height_meters:  # çŸ­é‚Šæ˜¯å¯¬åº¦æ–¹å‘\n",
    "            buffer_width_degrees = buffer_width_meters / meters_per_degree_lon\n",
    "        else:  # çŸ­é‚Šæ˜¯é«˜åº¦æ–¹å‘\n",
    "            buffer_width_degrees = buffer_width_meters / meters_per_degree_lat\n",
    "        \n",
    "        print(f\"    èª¿è©¦ä¿¡æ¯: çŸ©å½¢å°ºå¯¸ {width_meters:.1f}m Ã— {height_meters:.1f}m, L={L_meters:.1f}m, ç·©è¡å€={buffer_width_meters:.1f}m\")\n",
    "        # =======================================\n",
    "        \n",
    "        # å‰µå»ºåˆå§‹å‰–é¢ç·šï¼ˆæ²¿çŸ­é‚Šæ–¹å‘ï¼Œæ©«è·¨çŸ©å½¢ï¼‰\n",
    "        main_axis_unit = main_axis_vector / np.linalg.norm(main_axis_vector)\n",
    "        start_point = center_coords - main_axis_unit * np.linalg.norm(main_axis_vector) / 2\n",
    "        end_point = center_coords + main_axis_unit * np.linalg.norm(main_axis_vector) / 2\n",
    "        \n",
    "        # åœ¨GEEä¸­å‰µå»ºåˆå§‹å‰–é¢ç·š\n",
    "        initial_profile = ee.Geometry.LineString([start_point.tolist(), end_point.tolist()])\n",
    "        \n",
    "        # å‰µå»ºä½¿ç”¨åº¦æ•°å•ä½çš„ç·©è¡å€ï¼ˆå¯¬åº¦ç‚º2Lï¼‰\n",
    "        profile_buffer = initial_profile.buffer(buffer_width_degrees, error_margin)\n",
    "\n",
    "        # æ²¿åˆå§‹å‰–é¢ç·šæ¡æ¨£é»\n",
    "        num_sample_points = 50  # æ¡æ¨£é»æ•¸é‡\n",
    "        sample_points = []\n",
    "        mean_values = []\n",
    "        \n",
    "        for i in range(num_sample_points):\n",
    "            # æ²¿å‰–é¢ç·šçš„ä½ç½®åƒæ•¸ï¼ˆ0åˆ°1ï¼‰\n",
    "            t = i / (num_sample_points - 1)\n",
    "            sample_point = start_point + t * (end_point - start_point)\n",
    "            \n",
    "            # å‰µå»ºå‚ç·šæ–¹å‘çš„æ¡æ¨£ç·š\n",
    "            perpendicular_unit = perpendicular_vector / np.linalg.norm(perpendicular_vector)\n",
    "            perp_start = sample_point - perpendicular_unit * buffer_width_degrees / 2\n",
    "            perp_end = sample_point + perpendicular_unit * buffer_width_degrees / 2\n",
    "            \n",
    "            # å‰µå»ºå‚ç·š\n",
    "            perpendicular_line = ee.Geometry.LineString([perp_start.tolist(), perp_end.tolist()])\n",
    "            \n",
    "            # æ²¿å‚ç·šæ¡æ¨£SARå€¼\n",
    "            perp_sampled = flood_image.select('VV').reduceRegion(\n",
    "                reducer=ee.Reducer.toList(),\n",
    "                geometry=perpendicular_line,\n",
    "                scale=10,\n",
    "                maxPixels=1e10\n",
    "            )\n",
    "            \n",
    "            perp_values = perp_sampled.get('VV').getInfo()\n",
    "            if perp_values and len(perp_values) > 0:\n",
    "                # è¨ˆç®—å‚ç·šæ–¹å‘ä¸Šæ‰€æœ‰åƒç´ çš„å‡å€¼\n",
    "                mean_value = np.mean(perp_values)\n",
    "                sample_points.append(sample_point.tolist())\n",
    "                mean_values.append(mean_value)\n",
    "        \n",
    "        return {\n",
    "            'feature': feature,\n",
    "            'initial_profile': initial_profile,\n",
    "            'buffer_geom': profile_buffer,\n",
    "            'sample_points': sample_points,\n",
    "            'mean_values': mean_values,\n",
    "            'buffer_width_meters': buffer_width_meters,  # ä¿®å¾©ï¼šè¿”å›ç±³å–®ä½çš„ç·©è¡å€å¯¬åº¦\n",
    "            'buffer_width_degrees': buffer_width_degrees,  # èª¿è©¦ç”¨\n",
    "            'short_edge_length_meters': short_edge_length_meters,  # ä¿®å¾©ï¼šè¿”å›ç±³å–®ä½çš„çŸ­é‚Šé•·åº¦\n",
    "            'width_meters': width_meters,\n",
    "            'height_meters': height_meters\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"è™•ç†çŸ©å½¢æ™‚å‡ºéŒ¯: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# è™•ç†æ‰€æœ‰é€šéç¯©é¸çš„çŸ©å½¢\n",
    "print(\"\\n--- é–‹å§‹å‰µå»ºå‡å€¼å‰–é¢ç·š ---\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "filtered_list = filtered_boundingBoxes.toList(filtered_boundingBoxes.size())\n",
    "mean_profile_results = [] #è¿™æ˜¯ä¸€ä¸ªç”±å­—å…¸ç»„æˆçš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸€ä¸ªå‰–é¢çº¿çš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯\n",
    "\n",
    "for i in range(filtered_list.size().getInfo()):\n",
    "    print(f\"è™•ç†çŸ©å½¢ {i+1}...\")\n",
    "    server_feature = ee.Feature(filtered_list.get(i))\n",
    "    result = create_mean_profile_line(server_feature)\n",
    "    \n",
    "    if result is not None:\n",
    "        mean_profile_results.append(result)\n",
    "        print(f\"  âœ“ æˆåŠŸå‰µå»ºå‡å€¼å‰–é¢ç·šï¼Œæ¡æ¨£é»æ•¸: {len(result['mean_values'])}\")\n",
    "        print(f\"  âœ“ ç·©è¡å€å¯¬åº¦: {result['buffer_width_meters']:.2f}m\")\n",
    "        print(f\"  âœ“ SARå€¼ç¯„åœ: [{min(result['mean_values']):.2f}, {max(result['mean_values']):.2f}] dB\")\n",
    "    else:\n",
    "        print(f\"  âœ— è™•ç†å¤±æ•—\")\n",
    "\n",
    "print(f\"\\næˆåŠŸè™•ç† {len(mean_profile_results)} å€‹çŸ©å½¢çš„å‡å€¼å‰–é¢ç·š\")\n",
    "\n",
    "# å¯è¦–åŒ–å‡å€¼å‰–é¢ç·šç·©è¡å€\n",
    "if mean_profile_results:\n",
    "    # å°‡æ‰€æœ‰ç·©è¡å€æ·»åŠ åˆ°åœ°åœ–\n",
    "    buffer_collection = ee.FeatureCollection([\n",
    "        ee.Feature(result['buffer_geom']) \n",
    "        for result in mean_profile_results\n",
    "    ])\n",
    "    \n",
    "    buffer_outline = ee.Image().byte().paint(buffer_collection, 0, 1)\n",
    "    Map.addLayer(buffer_outline, {'palette': 'FFFF00'}, 'Mean Profile Buffer Zones')\n",
    "    \n",
    "    # æ·»åŠ åˆå§‹å‰–é¢ç·š\n",
    "    profile_collection = ee.FeatureCollection([\n",
    "        ee.Feature(result['initial_profile']) \n",
    "        for result in mean_profile_results\n",
    "    ])\n",
    "    \n",
    "    profile_outline = ee.Image().byte().paint(profile_collection, 0, 2)\n",
    "    Map.addLayer(profile_outline, {'palette': 'FF8C00'}, 'Initial Profile Lines')\n",
    "\n",
    "Map.centerObject(roi_choose1, 11)\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ee225",
   "metadata": {},
   "source": [
    "## 7.3 å‡å€¼å‰–é¢çº¿åƒç´ å€¼å„ç§å›¾è¡¨å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48464e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import pandas as pd\n",
    "\n",
    "# è§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n",
    "\n",
    "def create_smooth_profile_plots(mean_profile_results):\n",
    "    \"\"\"ä¸ºæ¯ä¸€æ¡å‡å€¼å‰–é¢çº¿åˆ›å»ºå¹³æ»‘çš„åƒç´ æŠ˜çº¿å›¾\n",
    "\n",
    "    Args:\n",
    "        mean_profile_results (_type_): å‡å€¼å‰–é¢çº¿ç»“æœåˆ—è¡¨\n",
    "    Returns:\n",
    "        æ¯ä¸€æ¡å‡å€¼å‰–é¢çº¿çš„å¹³æ»‘æŠ˜çº¿å›¾ï¼ŒåŒ…æ‹¬åŸå§‹æ•°æ®ç‚¹ã€å¹³æ»‘æ›²çº¿ã€å‡å€¼çº¿ã€Â±1æ ‡å‡†å·®åŒºåŸŸç­‰\n",
    "    \"\"\"\n",
    "    if not mean_profile_results:\n",
    "        print(\"æ²¡æœ‰æä¾›å‡å€¼å‰–é¢çº¿ç»“æœ\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # è®¡ç®—å­å›¾å¸ƒå±€\n",
    "    n_profiles = len(mean_profile_results)\n",
    "    cols = min(3, n_profiles) # æœ€å¤š3åˆ—\n",
    "    rows = (n_profiles + cols - 1) // cols # è®¡ç®—è¡Œæ•°\n",
    "    \n",
    "    # åˆ›å»ºå›¾å½¢\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows)) # åˆ›å»ºå­å›¾ç½‘æ ¼\n",
    "    \n",
    "    # å¤„ç†axesï¼ˆå­å›¾çš„ç´¢å¼•)ä¸ºä¸€ç»´æ•°ç»„ï¼Œæ–¹ä¾¿è¿­ä»£\n",
    "    if n_profiles== 1:\n",
    "        axes = [axes] # åªæœ‰ä¸€ä¸ªå­å›¾æ—¶ï¼Œaxesæ˜¯å•ä¸ªå¯¹è±¡ï¼Œè½¬ä¸ºåˆ—è¡¨\n",
    "    elif rows == 1:\n",
    "        axes = axes.flatten() # åªæœ‰ä¸€è¡Œæ—¶ï¼Œaxesæ˜¯ä¸€ç»´æ•°ç»„ï¼Œç›´æ¥æ‰å¹³åŒ–\n",
    "    else:\n",
    "        axes = axes.flatten() # å¤šè¡Œå¤šåˆ—æ—¶ï¼Œaxesæ˜¯äºŒç»´æ•°ç»„ï¼Œæ‰å¹³åŒ–ä¸ºä¸€ç»´æ•°ç»„\n",
    "        \n",
    "    # è®¾ç½®matplotlibæ ·å¼\n",
    "    plt.style.use('default') \n",
    "    \n",
    "    # éå†æ¯ä¸ªå‰–é¢çº¿ç»“æœï¼Œä¸ºæ¯ä¸ªåˆ›å»ºä¸€ä¸ªå­å›¾\n",
    "    for idx, result in enumerate(mean_profile_results):\n",
    "        ax = axes[idx] # è·å–å½“å‰å­å›¾\n",
    "        \n",
    "        #ä»ç»“æœå­—å…¸ä¸­æå–SARå€¼æ•°æ®\n",
    "        sar_values = np.array(result['mean_values']) #è½¬æ¢ä¸ºnumpyæ•°ç»„ä¾¿äºè®¡ç®—\n",
    "        sample_indices = np.arange(len(sar_values)) # åˆ›å»ºé‡‡æ ·ç‚¹ç´¢å¼•:0,1,2...,n-1\n",
    "        \n",
    "        # ç»˜åˆ¶åŸå§‹æ•°æ®ç‚¹å’Œè¿çº¿\n",
    "        ax.plot(sample_indices, sar_values, 'o-',  # 'o-'è¡¨ç¤ºåœ†ç‚¹+å®çº¿\n",
    "                color='lightgray',  # æµ…ç°è‰²\n",
    "                alpha=0.7,  # é€æ˜åº¦70%\n",
    "                linewidth=1,  # çº¿å®½1åƒç´ \n",
    "                markersize=4,  # æ ‡è®°ç‚¹å¤§å°4åƒç´ \n",
    "                label='Original Data',  # å›¾ä¾‹æ ‡ç­¾\n",
    "                zorder=1)  # ç»˜åˆ¶å±‚æ¬¡ï¼Œæ•°å€¼å°çš„åœ¨ä¸‹å±‚\n",
    "        \n",
    "        # ä½¿ç”¨ä¸‰æ¬¡æ ·æ¡æ’å€¼è¿›è¡Œå¹³æ»‘\n",
    "        if len(sar_values) > 3: #è‡³å°‘éœ€è¦å››ä¸ªç‚¹\n",
    "            try:\n",
    "                #åˆ›å»ºæ›´å¯†é›†çš„æ’å€¼ç‚¹ - åŸæ¥nä¸ªç‚¹æ‹“å±•åˆ°3nä¸ªç‚¹\n",
    "                x_new = np.linspace(0, len(sar_values)-1, num=len(sar_values)*3) #linspace()åœ¨æŒ‡å®šèŒƒå›´å†…ç”Ÿæˆå‡åŒ€é—´éš”çš„æ•°å€¼\n",
    "                \n",
    "                # åŸºäºåŸæ¥çš„åƒç´ ç´¢å¼•-saræ•°æ®ï¼Œä½¿ç”¨scipyçš„ä¸‰æ¬¡æ ·æ¡æ’å€¼,ç”Ÿæˆä¸€ä¸ªæ’å€¼å‡½æ•°\n",
    "                cs = interpolate.CubicSpline(sample_indices, # xåæ ‡ï¼ˆé‡‡æ ·ç‚¹ç´¢å¼•)\n",
    "                                             sar_values, # yåæ ‡ï¼ˆSARå€¼)\n",
    "                                            bc_type='natural') # è‡ªç„¶è¾¹ç•Œæ¡ä»¶\n",
    "                sar_smooth = cs(x_new) # è®¡ç®—æ’å€¼ç‚¹çš„SARå€¼\n",
    "                \n",
    "                # ç»˜åˆ¶å¹³æ»‘æ›²çº¿\n",
    "                ax.plot(x_new, sar_smooth, '-', # '-'è¡¨ç¤ºå®çº¿\n",
    "                        color = 'blue', # è“è‰²\n",
    "                        linewidth = 3,\n",
    "                        label = 'Cubic Spline Smoothing',\n",
    "                        zorder = 3) # ç»˜åˆ¶å±‚æ¬¡ï¼Œæ•°å€¼å¤§çš„åœ¨ä¸Šå±‚\n",
    "                \n",
    "            except Exception as e:\n",
    "                # å¦‚æœæ’å€¼å¤±è´¥ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯ä½†ä¸ä¸­æ–­ç¨‹åº\n",
    "                print(f\"Spline interpolation failed for profile {idx+1}: {e}\")\n",
    "                \n",
    "        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯\n",
    "        mean_val = np.mean(sar_values) # è®¡ç®—å‡å€¼\n",
    "        std_val = np.std(sar_values) # è®¡ç®—æ ‡å‡†å·®\n",
    "        min_val = np.min(sar_values) # è®¡ç®—æœ€å°å€¼\n",
    "        max_val = np.max(sar_values) # è®¡ç®—æœ€å¤§å€¼\n",
    "        \n",
    "        # æ·»åŠ æ°´å¹³å‚è€ƒçº¿\n",
    "        ax.axhline(y=mean_val,  # åœ¨y=mean_valä½ç½®ç”»æ°´å¹³çº¿\n",
    "                   color='red',  # çº¢è‰²\n",
    "                   linestyle='--',  # è™šçº¿æ ·å¼\n",
    "                   alpha=0.8,  # é€æ˜åº¦80%\n",
    "                   label=f'Mean: {mean_val:.1f} dB')  # å›¾ä¾‹æ ‡ç­¾ï¼Œä¿ç•™1ä½å°æ•°\n",
    "        \n",
    "        # æ·»åŠ Â±1æ ‡å‡†å·®çš„å‚è€ƒçº¿ï¼ˆä¸åŠ å›¾ä¾‹ï¼Œé¿å…å›¾ä¾‹è¿‡å¤šï¼‰\n",
    "        ax.axhline(y=mean_val + std_val, color='red', linestyle=':', alpha=0.6)  # ä¸Šç•Œ\n",
    "        ax.axhline(y=mean_val - std_val, color='red', linestyle=':', alpha=0.6)  # ä¸‹ç•Œ\n",
    "        \n",
    "        # å¡«å……Â±1æ ‡å‡†å·®åŒºåŸŸ\n",
    "        ax.fill_between(sample_indices,  # xåæ ‡èŒƒå›´\n",
    "                        mean_val - std_val,  # ä¸‹è¾¹ç•Œ\n",
    "                        mean_val + std_val,  # ä¸Šè¾¹ç•Œ\n",
    "                        alpha=0.2,  # é€æ˜åº¦20%\n",
    "                        color='red',  # çº¢è‰²å¡«å……\n",
    "                        label='Â±1Ïƒ Region')  # å›¾ä¾‹æ ‡ç­¾\n",
    "        \n",
    "        # è®¾ç½®å­å›¾æ ‡é¢˜ - åŒ…å«å‰–é¢çº¿ç¼–å·å’Œå…³é”®ä¿¡æ¯\n",
    "        ax.set_title(f'Profile {idx+1} - SAR Value Distribution\\n'  # ç¬¬ä¸€è¡Œï¼šæ ‡é¢˜\n",
    "                    f'Size: {result[\"width_meters\"]:.0f}m Ã— {result[\"height_meters\"]:.0f}m\\n'  # ç¬¬äºŒè¡Œï¼šçŸ©å½¢å°ºå¯¸\n",
    "                    f'Buffer: {result[\"buffer_width_meters\"]:.1f}m',  # ç¬¬ä¸‰è¡Œï¼šç¼“å†²åŒºå®½åº¦\n",
    "                    fontsize=11, pad=10)  # å­—ä½“å¤§å°11ï¼Œæ ‡é¢˜ä¸å›¾çš„é—´è·10\n",
    "        \n",
    "        # è®¾ç½®åæ ‡è½´æ ‡ç­¾\n",
    "        ax.set_xlabel('Sample Point Index', fontsize=10)  # xè½´æ ‡ç­¾\n",
    "        ax.set_ylabel('SAR Value (dB)', fontsize=10)  # yè½´æ ‡ç­¾\n",
    "        \n",
    "        # æ·»åŠ ç½‘æ ¼\n",
    "        ax.grid(True,  # å¯ç”¨ç½‘æ ¼\n",
    "                alpha=0.3,  # é€æ˜åº¦30%\n",
    "                linestyle='-',  # å®çº¿æ ·å¼\n",
    "                linewidth=0.5)  # çº¿å®½0.5åƒç´ \n",
    "        \n",
    "        # è®¾ç½®å›¾ä¾‹\n",
    "        ax.legend(loc='upper right',  # ä½ç½®ï¼šå³ä¸Šè§’\n",
    "                  fontsize=8,  # å­—ä½“å¤§å°8\n",
    "                  framealpha=0.9)  # å›¾ä¾‹æ¡†é€æ˜åº¦90%\n",
    "        \n",
    "        # åŠ¨æ€è°ƒæ•´Yè½´æ˜¾ç¤ºèŒƒå›´\n",
    "        y_range = max_val - min_val \n",
    "        ax.set_ylim(min_val - y_range*0.1, max_val + y_range*0.1)  # ä¸‹é™ä¸ºæœ€å°å€¼-10%èŒƒå›´ï¼Œä¸Šé™ä¸ºæœ€å¤§å€¼+10%èŒƒå›´\n",
    "        # éšè—å¤šä½™çš„å­å›¾ï¼ˆå½“å‰–é¢çº¿æ•°é‡ä¸èƒ½å®Œå…¨å¡«æ»¡ç½‘æ ¼æ—¶ï¼‰\n",
    "    for idx in range(n_profiles, len(axes)):\n",
    "        axes[idx].set_visible(False)  # è®¾ç½®ä¸ºä¸å¯è§\n",
    "    \n",
    "    # è°ƒæ•´å­å›¾å¸ƒå±€ï¼Œé¿å…é‡å \n",
    "    plt.tight_layout(pad=2.0)  # pad=2.0è®¾ç½®å­å›¾é—´çš„é—´è·\n",
    "    \n",
    "    # æ˜¾ç¤ºå›¾å½¢\n",
    "    plt.show()\n",
    "    \n",
    "def create_combined_profile_plot(mean_profile_results):\n",
    "    \"\"\"åˆ›å»ºæ‰€æœ‰å‰–é¢çº¿çš„ç»„åˆå¯¹æ¯”å›¾ï¼Œå°†æ‰€æœ‰å‰–é¢çº¿æ˜¾ç¤ºåœ¨åŒä¸€å¼ å›¾è¿›è¡Œæ¯”è¾ƒ\n",
    "\n",
    "    Args:\n",
    "        mean_profile_results (_type_): _description_\n",
    "    Returns:\n",
    "        ä¸€å¼ å¤§å›¾ï¼Œæ‰€æœ‰å‰–é¢çº¿çš„å¹³æ»‘æŠ˜çº¿å›¾è¿›è¡Œå¯¹æ¯”\n",
    "    \"\"\"\n",
    "    if not mean_profile_results:\n",
    "        print(\"æ²¡æœ‰æä¾›å‡å€¼å‰–é¢çº¿ç»“æœ\")\n",
    "        return\n",
    "    \n",
    "    # åˆ›å»ºå•ä¸ªå¤§å›¾\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªå‰–é¢çº¿åˆ†é…ä¸åŒé¢œè‰²\n",
    "    # np.linspaceç”Ÿæˆä»0åˆ°1çš„å‡åŒ€é—´éš”çš„æ•°å€¼ï¼Œæ•°é‡ä¸ºå‰–é¢çº¿æ•°é‡ï¼Œtab10å°†è¿™äº›æ•°ç»„æ˜ å°„åˆ°è°ƒè‰²æ¿ä¸­ï¼Œç”Ÿæˆé¢œè‰²æ•°ç»„\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(mean_profile_results)))\n",
    "    \n",
    "    # éå†æ¯ä¸ªå‰–é¢çº¿åŠå¯¹åº”é¢œè‰²\n",
    "    for idx, (result, color) in enumerate(zip(mean_profile_results, colors)):\n",
    "        sar_values = np.array(result['mean_values'])\n",
    "        sampled_indices = np.arange(len(sar_values))\n",
    "        \n",
    "        # å°†ç´¢å¼•æ­£è§„åŒ–åˆ°0-1èŒƒå›´ï¼Œä¾¿äºä¸åŒé•¿åº¦å‰–é¢çº¿è¿›è¡Œæ¯”è¾ƒ\n",
    "        normalized_indices =  sampled_indices / (len(sar_values) - 1) # ç´¢å¼•ä»0å¼€å§‹ï¼Œæœ€å¤§å€¼ä¸ºlen-1\n",
    "        \n",
    "        # ç»˜åˆ¶åŸå§‹æŠ˜çº¿æ•°æ®\n",
    "        # ç»˜åˆ¶åŸå§‹æ•°æ®ï¼ˆæ·¡è‰²ï¼Œç”¨äºå¯¹æ¯”ï¼‰\n",
    "        plt.plot(normalized_indices, sar_values, 'o-',\n",
    "                color=color, alpha=0.4,  # é€æ˜åº¦40%ï¼Œæ¯”è¾ƒæ·¡\n",
    "                linewidth=1, markersize=3,\n",
    "                label=f'Profile {idx+1} (Original)')\n",
    "        \n",
    "        # æ ·æ¡å¹³æ»‘å¤„ç†\n",
    "        if len(sar_values) > 3:\n",
    "            try:\n",
    "                # åˆ›å»ºæ­£è§„åŒ–çš„æ’å€¼ç‚¹\n",
    "                x_new = np.linspace(0, 1, len(sar_values)*2)\n",
    "                cs = interpolate.CubicSpline(normalized_indices, sar_values, bc_type='natural')\n",
    "                sar_smooth = cs(x_new)\n",
    "                \n",
    "                # ç»˜åˆ¶å¹³æ»‘æ›²çº¿ï¼ˆä¸é€æ˜ï¼Œçªå‡ºæ˜¾ç¤ºï¼‰\n",
    "                plt.plot(x_new, sar_smooth, '-',\n",
    "                        color=color, linewidth=3, alpha=0.9,\n",
    "                        label=f'Profile {idx+1} (Smoothed)')\n",
    "            except:\n",
    "                pass  # å¿½ç•¥æ’å€¼å¤±è´¥çš„æƒ…å†µ\n",
    "    \n",
    "    # è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œæ ‡ç­¾\n",
    "    plt.xlabel('Normalized Position', fontsize=12)  # xè½´ï¼šæ­£è§„åŒ–ä½ç½®\n",
    "    plt.ylabel('SAR Value (dB)', fontsize=12)  # yè½´ï¼šSARå€¼\n",
    "    plt.title('All Profile Lines Comparison', fontsize=14, pad=15)  # æ ‡é¢˜\n",
    "    plt.grid(True, alpha=0.3)  # æ·»åŠ ç½‘æ ¼\n",
    "    \n",
    "    # è®¾ç½®å›¾ä¾‹ï¼ˆæ”¾åœ¨å›¾å¤–å³ä¾§ï¼‰\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "def create_advanced_analysis_plots(mean_profile_results):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºè¿›é˜¶åˆ†æå›¾è¡¨ - æä¾›æ›´æ·±å…¥çš„ç»Ÿè®¡åˆ†æè§†å›¾\n",
    "    \"\"\"\n",
    "    if not mean_profile_results:\n",
    "        return\n",
    "    \n",
    "    # åˆ›å»º2x2çš„å­å›¾ç½‘æ ¼\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # æ”¶é›†æ‰€æœ‰æ•°æ®ç”¨äºåˆ†æ\n",
    "    all_values = []  # å­˜å‚¨æ‰€æœ‰å‰–é¢çº¿çš„æ‰€æœ‰SARå€¼\n",
    "    profile_stats = []  # å­˜å‚¨æ¯ä¸ªå‰–é¢çº¿çš„ç»Ÿè®¡ä¿¡æ¯\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªå‰–é¢çº¿çš„ç»Ÿè®¡æŒ‡æ ‡\n",
    "    for idx, result in enumerate(mean_profile_results):\n",
    "        values = np.array(result['mean_values'])\n",
    "        all_values.extend(values)  # å°†å½“å‰å‰–é¢çº¿çš„å€¼æ·»åŠ åˆ°æ€»åˆ—è¡¨\n",
    "        \n",
    "        # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯å­—å…¸\n",
    "        profile_stats.append({\n",
    "            'profile': f'Profile {idx+1}',\n",
    "            'mean': np.mean(values),  # å‡å€¼\n",
    "            'std': np.std(values),  # æ ‡å‡†å·®\n",
    "            'min': np.min(values),  # æœ€å°å€¼\n",
    "            'max': np.max(values),  # æœ€å¤§å€¼\n",
    "            'range': np.max(values) - np.min(values),  # å€¼åŸŸ\n",
    "            'buffer_width': result['buffer_width_meters']  # ç¼“å†²åŒºå®½åº¦\n",
    "        })\n",
    "    \n",
    "    # è½¬æ¢ä¸ºpandas DataFrameä¾¿äºæ“ä½œ\n",
    "    df_stats = pd.DataFrame(profile_stats)\n",
    "    \n",
    "    # å­å›¾1ï¼šæ•´ä½“SARå€¼åˆ†å¸ƒç›´æ–¹å›¾\n",
    "    ax1.hist(all_values,  # æ‰€æœ‰SARå€¼\n",
    "             bins=30,  # åˆ†ä¸º30ä¸ªåŒºé—´\n",
    "             alpha=0.7,  # é€æ˜åº¦70%\n",
    "             color='skyblue',  # å¤©è“è‰²\n",
    "             edgecolor='black')  # é»‘è‰²è¾¹æ¡†\n",
    "    ax1.set_xlabel('SAR Value (dB)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('SAR Value Distribution Histogram')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å­å›¾2ï¼šå„å‰–é¢çº¿çš„ç®±å½¢å›¾å¯¹æ¯”\n",
    "    box_data = [result['mean_values'] for result in mean_profile_results]  # æ¯ä¸ªå‰–é¢çº¿çš„æ•°æ®\n",
    "    box_labels = [f'Profile {i+1}' for i in range(len(mean_profile_results))]  # æ ‡ç­¾\n",
    "    ax2.boxplot(box_data, labels=box_labels)\n",
    "    ax2.set_ylabel('SAR Value (dB)')\n",
    "    ax2.set_title('SAR Value Box Plot by Profile')\n",
    "    ax2.tick_params(axis='x', rotation=45)  # xè½´æ ‡ç­¾æ—‹è½¬45åº¦\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å­å›¾3ï¼šç»Ÿè®¡æŒ‡æ ‡æŸ±çŠ¶å›¾å¯¹æ¯”\n",
    "    x_pos = np.arange(len(df_stats))  # xè½´ä½ç½®\n",
    "    width = 0.35  # æŸ±å­å®½åº¦\n",
    "    # ç»˜åˆ¶å¹¶æ’çš„æŸ±çŠ¶å›¾\n",
    "    ax3.bar(x_pos - width/2, df_stats['mean'], width, label='Mean', alpha=0.7)  # å‡å€¼\n",
    "    ax3.bar(x_pos + width/2, df_stats['std'], width, label='Std Dev', alpha=0.7)  # æ ‡å‡†å·®\n",
    "    ax3.set_xlabel('Profile')\n",
    "    ax3.set_ylabel('SAR Value (dB)')\n",
    "    ax3.set_title('Statistical Indicators Comparison')\n",
    "    ax3.set_xticks(x_pos)  # è®¾ç½®xè½´åˆ»åº¦ä½ç½®\n",
    "    ax3.set_xticklabels(df_stats['profile'])  # è®¾ç½®xè½´åˆ»åº¦æ ‡ç­¾\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å­å›¾4ï¼šç¼“å†²åŒºå®½åº¦vs SARå˜å¼‚æ€§æ•£ç‚¹å›¾\n",
    "    ax4.scatter(df_stats['buffer_width'],  # xï¼šç¼“å†²åŒºå®½åº¦\n",
    "               df_stats['std'],  # yï¼šæ ‡å‡†å·®ï¼ˆå˜å¼‚æ€§æŒ‡æ ‡ï¼‰\n",
    "               s=100,  # ç‚¹çš„å¤§å°\n",
    "               alpha=0.7,  # é€æ˜åº¦\n",
    "               c=range(len(df_stats)),  # é¢œè‰²æ˜ å°„åˆ°ç´¢å¼•\n",
    "               cmap='viridis')  # ä½¿ç”¨viridisè‰²å½©æ˜ å°„\n",
    "    ax4.set_xlabel('Buffer Width (m)')\n",
    "    ax4.set_ylabel('SAR Value Std Dev (dB)')\n",
    "    ax4.set_title('Buffer Width vs SAR Variability')\n",
    "    \n",
    "    # æ·»åŠ è¶‹åŠ¿çº¿ï¼ˆå¦‚æœæœ‰å¤šäº1ä¸ªç‚¹ï¼‰\n",
    "    if len(df_stats) > 1:\n",
    "        # ä½¿ç”¨ä¸€æ¬¡å¤šé¡¹å¼ï¼ˆç›´çº¿ï¼‰æ‹Ÿåˆ\n",
    "        z = np.polyfit(df_stats['buffer_width'], df_stats['std'], 1)\n",
    "        p = np.poly1d(z)  # åˆ›å»ºå¤šé¡¹å¼å‡½æ•°\n",
    "        ax4.plot(df_stats['buffer_width'], p(df_stats['buffer_width']),\n",
    "                \"r--\", alpha=0.8, linewidth=2)  # çº¢è‰²è™šçº¿\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªæ•£ç‚¹æ·»åŠ æ ‡ç­¾\n",
    "    for i, row in df_stats.iterrows():\n",
    "        ax4.annotate(f'P{i+1}',  # æ ‡ç­¾æ–‡æœ¬\n",
    "                    (row['buffer_width'], row['std']),  # æ ‡ç­¾ä½ç½®\n",
    "                    xytext=(5, 5),  # æ–‡æœ¬åç§»\n",
    "                    textcoords='offset points',  # åç§»å•ä½\n",
    "                    fontsize=8)\n",
    "    \n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # è°ƒæ•´æ•´ä½“å¸ƒå±€\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°ç»Ÿè®¡è¡¨æ ¼åˆ°æ§åˆ¶å°\n",
    "    print(\"\\n=== Profile Statistics Summary ===\")\n",
    "    print(df_stats.round(2).to_string(index=False))  # ä¿ç•™2ä½å°æ•°ï¼Œä¸æ˜¾ç¤ºè¡Œç´¢å¼•\n",
    "\n",
    "# æ‰§è¡Œå¯è§†åŒ–çš„ä¸»ç¨‹åº\n",
    "print(\"\\n=== Creating Mean Profile Line Visualization Charts ===\")\n",
    "\n",
    "# 1. åˆ›å»ºä¸ªåˆ«å‰–é¢çº¿çš„å¹³æ»‘å›¾\n",
    "print(\"1. Creating spline smoothed profile plots...\")\n",
    "create_smooth_profile_plots(mean_profile_results)\n",
    "\n",
    "# 2. åˆ›å»ºç»„åˆå¯¹æ¯”å›¾\n",
    "print(\"2. Creating combined comparison plot...\")\n",
    "create_combined_profile_plot(mean_profile_results)\n",
    "\n",
    "# 3. åˆ›å»ºè¿›é˜¶åˆ†æå›¾\n",
    "print(\"3. Creating advanced analysis plots...\")\n",
    "create_advanced_analysis_plots(mean_profile_results)\n",
    "\n",
    "print(\"All visualization charts completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6d699",
   "metadata": {},
   "source": [
    "## 7.4 å‡å€¼å‰–é¢çº¿æ¢¯åº¦è®¡ç®—åŠæ¢¯åº¦ç‚¹å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc13ec4",
   "metadata": {},
   "source": [
    "### å‡½æ•°1:ä»å‡å€¼å‰–é¢çº¿ç»“æœä¸­è®¡ç®—æ¯ä¸ªçŸ©å½¢å±€éƒ¨é˜ˆå€¼\n",
    "### å‡½æ•°2ï¼šæå–è®¡ç®—æˆåŠŸçš„ç»“æœä¸é˜ˆå€¼ç»Ÿè®¡ä¿¡æ¯\n",
    "### å‡½æ•°3ï¼šåœ¨åœ°å›¾ä¸Šå¯è§†åŒ–æ¢¯åº¦æå€¼ç‚¹å’Œé˜ˆå€¼è®¡ç®—ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702aa72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_local_threshold_from_mean_profile(mean_profile_result):\n",
    "    \"\"\"\n",
    "    ä»å‡å€¼å‰–é¢çº¿ç»“æœä¸­è®¡ç®—æ¯ä¸ªçŸ©å½¢çš„å±€éƒ¨é˜ˆå€¼\n",
    "    åŸç†:\n",
    "    1. ä»å‡å€¼å‰–é¢çº¿ä¸­æå–SARåºåˆ—\n",
    "    2. è®¡ç®—ä¸€é˜¶æ¢¯åº¦ï¼Œæ‰¾åˆ°æœ€å¤§æ­£æ¢¯åº¦å’Œæœ€å¤§è´Ÿæ¢¯åº¦\n",
    "    3. é€šè¿‡æ¢¯åº¦æå€¼ç‚¹ä¸¤ä¾§åƒç´ å€¼çš„å‡å€¼æ¥ç¡®å®šé˜ˆå€¼\n",
    "    4. æœ€ç»ˆé˜ˆå€¼ä¸ºæ­£è´Ÿæ¢¯åº¦é˜ˆå€¼çš„å¹³å‡å€¼\n",
    "    Args:\n",
    "        mean_profile_results: åŒ…å«å‡å€¼å‰–é¢çº¿ä¿¡æ¯çš„å­—å…¸\n",
    "         -'mean_values': SARå€¼åˆ—è¡¨\n",
    "         -'feature': åŸå§‹çŸ©å½¢Feature\n",
    "         -'neg/pos+grad_point_coords': æœ€æ­£/æœ€è´Ÿå…³é”®ç‚¹åæ ‡\n",
    "         -'local_threshold': å±€éƒ¨é˜ˆå€¼\n",
    "         -å…¶ä»–ç›¸å…³å±æ€§..\n",
    "         \n",
    "    Returns:\n",
    "        ä¸€ä¸ªåŒ…å«å±€éƒ¨é˜ˆå€¼å’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- 1.æ•°æ®å‡†å¤‡ä¸éªŒè¯ ---\n",
    "        sar_values = mean_profile_result.get('mean_values', []) # è·å–SARå€¼åˆ—è¡¨\n",
    "        sample_points = mean_profile_result.get('sample_points', []) # è·å–é‡‡æ ·ç‚¹åæ ‡åˆ—è¡¨\n",
    "        \n",
    "        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§\n",
    "        if not sar_values or len(sar_values) < 3:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'local_threshold':-999,\n",
    "                'reason':'NotEnoughPoints',\n",
    "                'error_msg':f'Least need 3 points, actually got {len(sar_values)}'\n",
    "            }\n",
    "        \n",
    "        # --- 2.æ¢¯åº¦è®¡ç®—ä¸åˆ†æ ---\n",
    "        #ä¸­å¿ƒå·®åˆ†çš„ä¸€é˜¶æ¢¯åº¦è®¡ç®—\n",
    "        sar_array = np.array(sar_values) \n",
    "        gradients = np.gradient(sar_array)\n",
    "        \n",
    "        print(f\" SARå€¼èŒƒå›´:[{sar_array.min():.2f}, {sar_array.max():.2f}] dB\")\n",
    "        print(f\" æ¢¯åº¦èŒƒå›´: [{gradients.min():.3f}, {gradients.max():.3f}] dB/pixel\")\n",
    "        \n",
    "        # --- 3.å¯»æ‰¾æå€¼æ¢¯åº¦ä½ç½® ---\n",
    "        # ä¸ºé¿å…è¾¹ç•Œæ•ˆåº”ï¼Œåœ¨é™¤å»é¦–å°¾çš„æ¢¯åº¦ä¸­å¯»æ‰¾æå€¼\n",
    "        inner_gradients = gradients[1:-1]\n",
    "        \n",
    "        # å¯»æ‰¾æœ€å¤§æ­£/è´Ÿæ¢¯åº¦åŠå…¶ç´¢å¼•\n",
    "        idx_max_neg_grad = np.argmin(inner_gradients) + 1 #å› ä¸ºè¿›è¡Œäº†è¾¹ç•Œï¼ˆå¼€å¤´ä¸€ä¸ªåƒç´ ï¼‰å‰”é™¤ï¼Œæ‰€ä»¥ç´¢å¼•éœ€è¦+1\n",
    "        idx_max_pos_grad = np.argmax(inner_gradients) + 1\n",
    "        \n",
    "        print(f\"    æœ€å¤§è´Ÿæ¢¯åº¦: ä½ç½®{idx_max_neg_grad}, å€¼={gradients[idx_max_neg_grad]:.3f}\")\n",
    "        print(f\"    æœ€å¤§æ­£æ¢¯åº¦: ä½ç½®{idx_max_pos_grad}, å€¼={gradients[idx_max_pos_grad]:.3f}\")\n",
    "        \n",
    "        # --- 4.è®¡ç®—å±€éƒ¨é˜ˆå€¼ ---\n",
    "        # ä½¿ç”¨ä¸¤ä¸ªæ¢¯åº¦æå€¼ç‚¹ä¸å…¶é‚»è¿‘ç‚¹çš„å‡å€¼ä½œä¸ºé˜ˆå€¼ï¼Œä¸¤ä¸ªå‡å€¼çš„å†å‡å€¼ä¸ºæœ€ç»ˆé˜ˆå€¼\n",
    "        if idx_max_neg_grad > 0:\n",
    "            thresh_neg = (sar_array[idx_max_neg_grad] + sar_array[idx_max_neg_grad -1]) / 2.0\n",
    "        else:\n",
    "            thresh_neg = sar_array[idx_max_neg_grad]    #è¾¹ç•Œç‚¹ï¼Œæ— æ³•å–å‰ä¸€ä¸ªç‚¹\n",
    "        \n",
    "        if idx_max_pos_grad > 0:\n",
    "            thresh_pos = (sar_array[idx_max_pos_grad] + sar_array[idx_max_pos_grad - 1]) / 2.0\n",
    "        else:\n",
    "            thresh_pos = sar_array[idx_max_pos_grad]\n",
    "            \n",
    "        # æœ€ç»ˆé˜ˆå€¼ï¼šä¸¤ä¸ªå€™é€‰é˜ˆå€¼çš„å¹³å‡\n",
    "        local_threshold = (thresh_neg + thresh_pos) / 2.0\n",
    "        \n",
    "        print(f\"    è´Ÿæ¢¯åº¦é˜ˆå€¼: {thresh_neg:.2f} dB\")\n",
    "        print(f\"    æ­£æ¢¯åº¦é˜ˆå€¼: {thresh_pos:.2f} dB\")\n",
    "        print(f\"    æœ€ç»ˆå±€éƒ¨é˜ˆå€¼: {local_threshold:.2f} dB\")\n",
    "        \n",
    "        # ---5.æ ¹æ®ä¹‹å‰çš„å…³é”®é‡‡æ ·ç‚¹æå–å…³é”®ç‚¹åæ ‡ ---\n",
    "        neg_grad_coords = sample_points[idx_max_neg_grad] if idx_max_neg_grad < len(sample_points) else None\n",
    "        pos_grad_coords = sample_points[idx_max_pos_grad] if idx_max_pos_grad < len(sample_points) else None\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'local_threshold': local_threshold,\n",
    "            'thresh_neg': thresh_neg,\n",
    "            'thresh_pos': thresh_pos,\n",
    "            'max_neg_gradient': gradients[idx_max_neg_grad],\n",
    "            'max_pos_gradient': gradients[idx_max_pos_grad],\n",
    "            'neg_grad_point_coords': neg_grad_coords,\n",
    "            'pos_grad_point_coords': pos_grad_coords,\n",
    "            'neg_grad_index': idx_max_neg_grad,\n",
    "            'pos_grad_index': idx_max_pos_grad,\n",
    "            'all_gradients': gradients.tolist(),\n",
    "            'reason': f\"æˆåŠŸè®¡ç®—ã€‚è´Ÿæ¢¯åº¦é˜ˆå€¼={thresh_neg:.2f}, æ­£æ¢¯åº¦é˜ˆå€¼={thresh_pos:.2f}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'local_threshold': -999,\n",
    "            'reason': 'CalculationError',\n",
    "            'error_msg': str(e)\n",
    "        }\n",
    "        \n",
    "def process_all_profile_thresholds(mean_profile_results):\n",
    "    \"\"\"\n",
    "    å¤„ç†æ‰€æœ‰å‡å€¼å‰–é¢çº¿ï¼Œè®¡ç®—æ¯ä¸ªçŸ©å½¢çš„å±€éƒ¨é˜ˆå€¼,å¹¶æ‰“å°å‡ºå…·ä½“ä¿¡æ¯ï¼Œè¿™ä¸ªå‡½æ•°è¦åœ¨ä¸Šä¸ªå‡½æ•°çš„åŸºç¡€ä¸Šè¿è¡Œ\n",
    "    Args:\n",
    "        mean_profile_results: åŒ…å«æ‰€æœ‰å‡å€¼å‰–é¢çº¿ç»“æœçš„åˆ—è¡¨\n",
    "    Returns:\n",
    "        tuple:æˆåŠŸçš„é˜ˆå€¼ç»“æœåˆ—è¡¨ï¼Œæ‰€æœ‰ç»“æœçš„ç»Ÿè®¡ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"å¼€å§‹æ‰¹é‡è®¡ç®—å‡å€¼å‰–é¢çº¿çš„å±€éƒ¨é˜ˆå€¼\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Profile ID':<12} | {'Local Threshold':<16} | {'Status':<10} | {'Details'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    successful_results = []\n",
    "    failed_results = []\n",
    "    all_thresholds = []\n",
    "    \n",
    "    for idx, profile_result in enumerate(mean_profile_results):\n",
    "        print(f\"\\nå¤„ç†å‰–é¢çº¿{idx+1}...\")\n",
    "        \n",
    "        # è°ƒç”¨é˜ˆå€¼è®¡ç®—å‡½æ•°\n",
    "        threshold_result = calculate_local_threshold_from_mean_profile(profile_result)\n",
    "        \n",
    "        # åˆå¹¶åŸå§‹å‰–é¢çº¿æ•°æ®å’Œé˜ˆå€¼è®¡ç®—ç»“æœ\n",
    "        # è¿™æ˜¯ä¸ªå¤§å­—å…¸ï¼Œå…¶ä¸­å±æ€§åµŒå¥—å°å­—å…¸\n",
    "        combined_result = {\n",
    "            'profile_id' : idx + 1,  # å‰–é¢çº¿IDï¼Œint\n",
    "            'original_profile_data' : profile_result, #åŸå§‹å‰–é¢çº¿æ•°æ®ï¼Œå­—å…¸\n",
    "            'threshold_calculation' : threshold_result #é˜ˆå€¼è®¡ç®—ç»“æœï¼Œå­—å…¸\n",
    "        }\n",
    "        \n",
    "        # æ ¹æ®è®¡ç®—ç»“æœåˆ†ç±»\n",
    "        if threshold_result['success']:\n",
    "            threshold_value = threshold_result['local_threshold'] #æå–è¯¥çŸ©å½¢çš„å±€éƒ¨é˜ˆå€¼\n",
    "            status = 'SUCCESS'\n",
    "            details = f\"threshold={threshold_value:.2f} dB\"\n",
    "            successful_results.append(combined_result) # æ·»åŠ åˆ°æˆåŠŸåˆ—è¡¨\n",
    "            all_thresholds.append(threshold_value) # ä»…ä¿å­˜é˜ˆå€¼ç”¨äºç»Ÿè®¡\n",
    "        else:\n",
    "            threshold_value = \"N/A\" # å¤±è´¥æ—¶æ— é˜ˆå€¼\n",
    "            status = 'FAILED'\n",
    "            details = f\"Reason: {threshold_result.get('reason','Unknown error')}\"\n",
    "            failed_results.append(combined_result) # æ·»åŠ åˆ°å¤±è´¥åˆ—è¡¨\n",
    "            \n",
    "        # æ‰“å°å¤„ç†ç»“æœ\n",
    "        print(f\"Profile{idx+1:<5}|{str(threshold_value):<16}|{status:<10}{details}\") # åˆ†åˆ«æ‰“å°IDã€é˜ˆå€¼ã€çŠ¶æ€ã€è¯¦æƒ…ä¿¡æ¯\n",
    "        \n",
    "        # å¦‚æœæˆåŠŸï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯\n",
    "        if threshold_result['success']:\n",
    "            print(f\"    âœ“ è´Ÿæ¢¯åº¦: {threshold_result['max_neg_gradient']:.3f} â†’ é˜ˆå€¼ {threshold_result['thresh_neg']:.2f}\")\n",
    "            print(f\"    âœ“ æ­£æ¢¯åº¦: {threshold_result['max_pos_gradient']:.3f} â†’ é˜ˆå€¼ {threshold_result['thresh_pos']:.2f}\")\n",
    "            \n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # ---ç»Ÿè®¡ä¿¡æ¯æ±‡æ€»---\n",
    "    total_profiles = len(mean_profile_results) # å‰–é¢çº¿æ•°é‡\n",
    "    success_count = len(successful_results) # æˆåŠŸæ•°é‡\n",
    "    failure_count = len(failed_results) # å¤±è´¥æ•°é‡\n",
    "    \n",
    "    #ç»§ç»­åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œå­˜å‚¨å„ä¸ªç»Ÿè®¡å€¼\n",
    "    statistics = {\n",
    "        'total_profiles':total_profiles,\n",
    "        'success_count':success_count,\n",
    "        'failure_count':failure_count,\n",
    "        'success_rate': (success_count / total_profiles * 100) if total_profiles > 0 else 0,\n",
    "        'all_thresholds': all_thresholds\n",
    "    }\n",
    "    \n",
    "    # å¦‚æœæœ‰æˆåŠŸçš„é˜ˆå€¼ï¼Œè®¡ç®—ç»Ÿè®¡æŒ‡æ ‡\n",
    "    if all_thresholds:\n",
    "        statistics.update({\n",
    "            'mean_threshold':np.mean(all_thresholds),\n",
    "            'std_threshold':np.std(all_thresholds),\n",
    "            'min_threshold':np.min(all_thresholds),\n",
    "            'max_threshold':np.max(all_thresholds),\n",
    "            'threshold_range':np.max(all_thresholds) - np.min(all_thresholds)\n",
    "        })\n",
    "        print(f\"\\né˜ˆå€¼ç»Ÿè®¡æ‘˜è¦:\")\n",
    "        print(f\"  æˆåŠŸç‡: {statistics['success_rate']:.1f}% ({success_count}/{total_profiles})\")\n",
    "        print(f\"  é˜ˆå€¼èŒƒå›´: [{statistics['min_threshold']:.2f}, {statistics['max_threshold']:.2f}] dB\")\n",
    "        print(f\"  å¹³å‡é˜ˆå€¼: {statistics['mean_threshold']:.2f} Â± {statistics['std_threshold']:.2f} dB\")\n",
    "    else:\n",
    "        print(f\"\\nå¤„ç†ç»“æœ: å¤±è´¥ç‡100% (0/{total_profiles})\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return successful_results, statistics # è¿”å›æˆåŠŸçš„ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯\n",
    "   \n",
    "\n",
    "def visualize_threshold_points_on_map(successful_results, Map):\n",
    "    \"\"\"\n",
    "    åœ¨åœ°å›¾ä¸Šå¯è§†åŒ–æ¢¯åº¦æå€¼ç‚¹å’Œé˜ˆå€¼è®¡ç®—ç»“æœ\n",
    "    \n",
    "    Args:\n",
    "        successful_results(list):æˆåŠŸè®¡ç®—é˜ˆå€¼çš„ç»“æœåˆ—è¡¨\n",
    "        Map(geemap.Map): geemapåœ°å›¾å¯¹è±¡\n",
    "    \"\"\"\n",
    "    if not successful_results:\n",
    "        print(\"æ²¡æœ‰æˆåŠŸçš„é˜ˆå€¼è®¡ç®—ç»“æœä¾›å¯è§†åŒ–\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nåœ¨åœ°å›¾ä¸Šå¯è§†åŒ–{len(successful_results)}ä¸ªå‰–é¢çº¿çš„æ¢¯åº¦æå€¼ç‚¹...\")\n",
    "    \n",
    "    # æ”¶é›†æ‰€æœ‰æ¢¯åº¦æå€¼ç‚¹çš„åæ ‡ï¼Œåˆ›å»ºä¸¤ä¸ªlist\n",
    "    neg_grad_points = [] #æœ€å¤§è´Ÿæ¢¯åº¦ç‚¹\n",
    "    pos_grad_points = [] #æœ€å¤§æ­£æ¢¯åº¦ç‚¹\n",
    "    \n",
    "    for result in successful_results: # éå†æ¯ä¸ªæˆåŠŸç»“æœ(ä¸ºcombined_resultsçš„åµŒå¥—å­—å…¸)\n",
    "        threshold_calc = result['threshold_calculation'] # æå–åµŒå¥—å­—å…¸ä¸­çš„é˜ˆå€¼è®¡ç®—ç»“æœå­—å…¸\n",
    "        \n",
    "        # æå–æ­£ï¼Œè´Ÿæ¢¯åº¦æå€¼ç‚¹åæ ‡åˆ°åˆ—è¡¨ä¸­\n",
    "        neg_coords = threshold_calc.get('neg_grad_point_coords')\n",
    "        if neg_coords:\n",
    "            neg_grad_points.append(ee.Feature(ee.Geometry.Point(neg_coords))) # åˆ›å»ºgeeçŸ¢é‡ç±»å‹çš„ç‚¹Featureå¹¶æ·»åŠ åˆ°åˆ—è¡¨\n",
    "        pos_coords = threshold_calc.get('pos_grad_point_coords')\n",
    "        if pos_coords:\n",
    "            pos_grad_points.append(ee.Feature(ee.Geometry.Point(pos_coords)))\n",
    "            \n",
    "    # åˆ›å»ºFeatureCollectionå¹¶æ·»åŠ åˆ°åœ°å›¾\n",
    "    if neg_grad_points:\n",
    "        neg_points_fc = ee.FeatureCollection(neg_grad_points)\n",
    "        Map.addLayer(neg_points_fc,{\n",
    "            'color':'008000',\n",
    "            'pointSize':8,\n",
    "            'pointShape':'circle'\n",
    "        }, 'Max Negative Gradient Points')\n",
    "        print(f\"  âœ“ æ·»åŠ  {len(neg_grad_points)} ä¸ªæœ€å¤§è´Ÿæ¢¯åº¦ç‚¹-red\")\n",
    "        \n",
    "    if pos_grad_points:\n",
    "        pos_points_fc = ee.FeatureCollection(pos_grad_points)\n",
    "        Map.addLayer(pos_points_fc,{\n",
    "            'color':'0000FF',\n",
    "            'pointSize':8,\n",
    "            'pointShape':'circle'\n",
    "        }, 'Max Positive Gradient Points')\n",
    "        print(f\"  âœ“ æ·»åŠ  {len(pos_grad_points)} ä¸ªæœ€å¤§æ­£æ¢¯åº¦ç‚¹-blue\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22e4cd",
   "metadata": {},
   "source": [
    "### ä¸»æ‰§è¡Œæµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥å‰ææ¡ä»¶\n",
    "if 'mean_profile_results' not in globals() or not mean_profile_results:\n",
    "    print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° 'mean_profile_results' æ•°æ®\")\n",
    "    print(\"   è¯·ç¡®ä¿æ‚¨å·²æˆåŠŸè¿è¡Œäº†ç¬¬7.2èŠ‚çš„å‡å€¼å‰–é¢çº¿æå–ä»£ç \")\n",
    "else:\n",
    "    print(f\"âœ“ æ‰¾åˆ° {len(mean_profile_results)} æ¡å‡å€¼å‰–é¢çº¿ï¼Œå¼€å§‹è®¡ç®—å±€éƒ¨é˜ˆå€¼...\")\n",
    "    \n",
    "    # æ‰§è¡Œæ‰¹é‡é˜ˆå€¼è®¡ç®—\n",
    "    successful_threshold_results, threshold_statistics = process_all_profile_thresholds(mean_profile_results)\n",
    "    \n",
    "    # åœ¨åœ°å›¾ä¸Šå¯è§†åŒ–ç»“æœ\n",
    "    Map_gradient = geemap.Map()\n",
    "    Map_gradient.centerObject(roi_choose1, 11)\n",
    "    \n",
    "    if successful_threshold_results:\n",
    "        visualize_threshold_points_on_map(successful_threshold_results, Map ) # æ–°åœ°å›¾å°±ç”¨ä¸Šé¢çš„Map_gradient\n",
    "        \n",
    "        # å°†ç»“æœä¿å­˜ä¸ºå…¨å±€å˜é‡ï¼Œä¾›åç»­ä½¿ç”¨\n",
    "        global local_threshold_results\n",
    "        local_threshold_results = successful_threshold_results\n",
    "        \n",
    "        print(f\"\\nğŸ‰ å±€éƒ¨é˜ˆå€¼è®¡ç®—å®Œæˆï¼\")\n",
    "        print(f\"   âœ“ æˆåŠŸè®¡ç®— {len(successful_threshold_results)} ä¸ªå±€éƒ¨é˜ˆå€¼\")\n",
    "        print(f\"   âœ“ ç»“æœå·²ä¿å­˜åˆ°å…¨å±€å˜é‡ 'local_threshold_results'\")\n",
    "        print(f\"   âœ“ æ¢¯åº¦æå€¼ç‚¹å·²åœ¨åœ°å›¾ä¸Šå¯è§†åŒ–\")\n",
    "        \n",
    "        # å±…ä¸­æ˜¾ç¤ºåˆ°ç ”ç©¶åŒºåŸŸ\n",
    "        if 'roi_choose1' in globals():\n",
    "            Map.centerObject(roi_choose1, 12)\n",
    "    else:\n",
    "        print(\"âŒ æ²¡æœ‰æˆåŠŸè®¡ç®—å‡ºä»»ä½•å±€éƒ¨é˜ˆå€¼\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# å±€éƒ¨é˜ˆå€¼è®¡ç®—æ¨¡å—æ‰§è¡Œå®Œæ¯•\")  \n",
    "print(\"#\"*80)\n",
    "\n",
    "# æ˜¾ç¤ºæ›´æ–°åçš„åœ°å›¾\n",
    "Map\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f259f",
   "metadata": {},
   "source": [
    "# 8.ç»“åˆå±€éƒ¨æœ€ä¼˜å’Œå…¨å±€æœ€ä¼˜çš„æ´ªæ°´æå–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e0c822",
   "metadata": {},
   "source": [
    "## 8.1 è¿‡æ»¤é˜ˆå€¼è®¡ç®—ç»“æœå¤ªå¤§çš„çŸ©å½¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_threshold_results(successful_threshold_results, threshold_limit):\n",
    "    \"\"\"\n",
    "    è¿‡æ»¤å±€éƒ¨é˜ˆå€¼ç»“æœï¼Œç§»é™¤é˜ˆå€¼å¤§äºæŒ‡å®šé™åˆ¶çš„çŸ©å½¢\n",
    "    \n",
    "    Args:\n",
    "        successful_threshold_results (dict): æˆåŠŸè®¡ç®—é˜ˆå€¼çš„ç»“æœåµŒå¥—å­—å…¸\n",
    "        threshold_limit (float): é˜ˆå€¼ä¸Šé™ï¼Œå•ä½dB\n",
    "    Returns:\n",
    "        filtered_results(dict): è¿‡æ»¤åçš„ç»“æœåµŒå¥—å­—å…¸\n",
    "        statistics(dict): è¿‡æ»¤åçš„ç»“æœåˆ—è¡¨ï¼Œç»Ÿè®¡ä¿¡æ¯å­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- start filtering local threshold(remove threshold > {threshold_limit}db)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Profile ID':<12} | {'Local Threshold':<16} | {'Status':<10} | {'Action'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    filtered_results = [] # å­˜å‚¨é€šè¿‡è¿‡æ»¤çš„ç»“æœ\n",
    "    removed_results = [] # å­˜å‚¨è¢«ç§»é™¤çš„ç»“æœ\n",
    "    valid_thresholds = [] # å­˜å‚¨é€šè¿‡è¿‡æ»¤çš„é˜ˆå€¼ï¼Œç”¨äºç»Ÿè®¡\n",
    "    \n",
    "    for result in successful_threshold_results: # éå†æ¯ä¸ªæˆåŠŸç»“æœé‡Œé¢çš„åµŒå¥—å­—å…¸\n",
    "        profile_id = result['profile_id'] # æå–å‰–é¢çº¿ID,int\n",
    "        threshold_calc = result['threshold_calculation'] # æå–é˜ˆå€¼è®¡ç®—ç»“æœå­—å…¸,dict\n",
    "        local_threshold = threshold_calc['local_threshold'] # æå–å±€éƒ¨é˜ˆå€¼,float\n",
    "        \n",
    "        if local_threshold <= threshold_limit:\n",
    "            # ä¿ç•™å½“å‰çŸ©å½¢\n",
    "            filtered_results.append(result)\n",
    "            valid_thresholds.append(local_threshold) # æ·»åŠ åˆ°æœ‰æ•ˆé˜ˆå€¼åˆ—è¡¨\n",
    "            status = \"KEPT\"\n",
    "            action = f\"Valic threshold: {local_threshold:.2f} \"\n",
    "        else:\n",
    "            # ç§»é™¤çŸ©å½¢\n",
    "            removed_results.append(result)\n",
    "            status = \"REMOVED\"\n",
    "            action = f\"Threshold {local_threshold:.2f} > {threshold_limit}\"\n",
    "        print(f\"Profile{profile_id:<7} | {local_threshold:<16.2f} | {status:<10} | {action}\")\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(f\"è¿‡æ»¤ç»“æœ:\")\n",
    "    print(f\"  âœ“ ä¿ç•™çŸ©å½¢: {len(filtered_results)} ä¸ª\")\n",
    "    print(f\"  âœ— ç§»é™¤çŸ©å½¢: {len(removed_results)} ä¸ª\")\n",
    "    \n",
    "    if valid_thresholds:\n",
    "        statistics = {\n",
    "            'total_kept': len(filtered_results),\n",
    "            'total_removed': len(removed_results),\n",
    "            'valid_thresholds': valid_thresholds,\n",
    "            'mean_threshold': np.mean(valid_thresholds),\n",
    "            'std_threshold': np.std(valid_thresholds),\n",
    "            'min_threshold': np.min(valid_thresholds),\n",
    "            'max_threshold': np.max(valid_thresholds)\n",
    "        }\n",
    "        print(f\"  é˜ˆå€¼ç»Ÿè®¡: å‡å€¼={statistics['mean_threshold']:.2f}, \"\n",
    "              f\"èŒƒå›´=[{statistics['min_threshold']:.2f}, {statistics['max_threshold']:.2f}] dB\")\n",
    "    else:\n",
    "        statistics = {\n",
    "            'total_kept': 0,\n",
    "            'total_removed': len(removed_results),\n",
    "            'valid_thresholds': [],\n",
    "            'mean_threshold': None\n",
    "        }\n",
    "        print(\"  âš ï¸  è­¦å‘Šï¼šæ²¡æœ‰ä»»ä½•çŸ©å½¢é€šè¿‡é˜ˆå€¼è¿‡æ»¤\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    return filtered_results, statistics\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab02d9",
   "metadata": {},
   "source": [
    "## 8.2 è‡ªé€‚åº”æ´ªæ°´æå–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167917d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adaptive_flood_extraction(filtered_threshold_results, flood_image, roi_choose1):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºè‡ªé€‚åº”æ´ªæ°´æå–:çŸ©å½¢å†…éƒ¨ä½¿ç”¨é˜ˆå€¼ï¼Œå¤–éƒ¨ä½¿ç”¨å…¨å±€å¹³å‡é˜ˆå€¼\n",
    "    Args:\n",
    "        filtered_threshold_results (dict): è¿‡æ»¤é˜ˆå€¼åçš„åµŒå¥—å­—å…¸\n",
    "        flood_image (ee.Image): ç”¨äºæ´ªæ°´æå–çš„SARå½±åƒ\n",
    "        roi_choose1 (ee.Geometry): ç ”ç©¶åŒºåŸŸ\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- å¼€å§‹è‡ªé€‚åº”æ´ªæ°´æå– ---\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not filtered_threshold_results:\n",
    "        print(\"âŒ é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„å±€éƒ¨é˜ˆå€¼ç»“æœè¿›è¡Œæ´ªæ°´æå–\")\n",
    "        return None, None\n",
    "    \n",
    "    # --- 1.è®¡ç®—å…¨å±€å¹³å‡é˜ˆå€¼ ---\n",
    "    local_thresholds = [\n",
    "        result['threshold_calculation']['local_threshold'] \n",
    "        for result in filtered_threshold_results # æå–æ‰€æœ‰å±€éƒ¨é˜ˆå€¼åˆ°åˆ—è¡¨ä¸­\n",
    "    ]\n",
    "    \n",
    "    # è®¡ç®—å…¨å±€å¹³å‡é˜ˆå€¼\n",
    "    global_average_threshold = np.mean(local_thresholds)\n",
    "    \n",
    "    print(f\"æ­¥éª¤1: è®¡ç®—å…¨å±€å¹³å‡é˜ˆå€¼\")\n",
    "    print(f\"  âœ“ ä½¿ç”¨ {len(local_thresholds)} ä¸ªæœ‰æ•ˆå±€éƒ¨é˜ˆå€¼\")\n",
    "    print(f\"  âœ“ å…¨å±€å¹³å‡é˜ˆå€¼: {global_average_threshold:.2f} dB\")\n",
    "    print(f\"  âœ“ å±€éƒ¨é˜ˆå€¼èŒƒå›´: [{min(local_thresholds):.2f}, {max(local_thresholds):.2f}] dB\")\n",
    "    \n",
    "    # --- 2.ä¸ºæ¯ä¸ªçŸ©å½¢åˆ›å»ºå±€éƒ¨æ´ªæ°´æ·¹æ²¡æ©æ¨¡ ---\n",
    "    print(f\"\\næ­¥éª¤2: ä¸ºæ¯ä¸ªçŸ©å½¢åº”ç”¨å±€éƒ¨é˜ˆå€¼\")\n",
    "    \n",
    "    local_flood_images = []\n",
    "    rectangle_geometries = []\n",
    "    \n",
    "    for i,result in enumerate(filtered_threshold_results): # éå†æ¯ä¸ªæˆåŠŸç»“æœé‡Œé¢çš„åµŒå¥—å­—å…¸\n",
    "        profile_id = result['profile_id'] # æå–å‰–é¢çº¿ID,int\n",
    "        threshold_calc = result['threshold_calculation'] # æå–é˜ˆå€¼è®¡ç®—ç»“æœå­—å…¸,dict\n",
    "        local_threshold = threshold_calc['local_threshold'] # æå–å±€éƒ¨é˜ˆå€¼,float\n",
    "        rectangle_feature = result['original_profile_data']['feature'] # æå–åŸå§‹çŸ©å½¢Feature,ee.Feature\n",
    "        \n",
    "        # è·å–çŸ©å½¢çš„å‡ ä½•å½¢çŠ¶\n",
    "        rectangle_geom = rectangle_feature.geometry()\n",
    "        rectangle_geometries.append(rectangle_geom) # ä¿å­˜çŸ©å½¢å‡ ä½•ç”¨äºåç»­å¯è§†åŒ–\n",
    "        \n",
    "        # åœ¨è¯¥çŸ©å½¢å†…åº”ç”¨å±€éƒ¨é˜ˆå€¼\n",
    "        # åˆ›å»ºçŸ©å½¢å†…çš„æ´ªæ°´æ©æ¨¡\n",
    "        local_flood_mask = flood_image.lt(local_threshold) # SARå€¼ä½äºå±€éƒ¨\n",
    "        \n",
    "        # å°†æ´ªæ°´æ©æ¨¡è£å‰ªåˆ°å½“å‰çŸ©å½¢èŒƒå›´\n",
    "        local_flood_clipped = local_flood_mask.clip(rectangle_geom).selfMask() # selfMask()å°†éæ´ªæ°´åŒºåŸŸè®¾ä¸ºé€æ˜\n",
    "        local_flood_images.append(local_flood_clipped) # è¿™ä¸ªåˆ—è¡¨åŒ…å«äº†æ‰€æœ‰çš„å±€éƒ¨å½±åƒ-å³åªæœ‰çŸ©å½¢å†…çš„æ´ªæ°´ä¸º1,å…¶ä»–åŒºåŸŸä¸º0æˆ–æ— å€¼\n",
    "        \n",
    "        print(f\"  Profile {profile_id}: å±€éƒ¨é˜ˆå€¼ {local_threshold:.2f} dB â†’ çŸ©å½¢å†…æ´ªæ°´æå–\")\n",
    "    \n",
    "    # --- 3.åˆå¹¶æ‰€æœ‰å±€éƒ¨æ´ªæ°´å›¾ ---\n",
    "    print(f\"\\næ­¥éª¤3ï¼šåˆå¹¶æ‰€æœ‰çŸ©å½¢å†…çš„å±€éƒ¨æ´ªæ°´å›¾\")\n",
    "    \n",
    "    # å°†æ‰€æœ‰å±€éƒ¨æ´ªæ°´å›¾åƒåˆå¹¶\n",
    "    if len(local_flood_images) == 1:\n",
    "        all_local_floods = local_flood_images[0]\n",
    "    else:\n",
    "        local_floods_collection = ee.ImageCollection(local_flood_images) # æŠŠä¹‹å‰çš„å±€éƒ¨å½±åƒèšåˆä¸ºä¸€ä¸ªå½±åƒé›†åˆ\n",
    "        all_local_floods = local_floods_collection.mosaic() # ä½¿ç”¨mosaicæ–¹æ³•åˆå¹¶å½±åƒé›†åˆ,å³åœ¨æ¯ä¸ªåƒç´ å–æœ€å¤§å€¼\n",
    "    \n",
    "    print(f\"  âœ“ åˆå¹¶äº† {len(local_flood_images)} ä¸ªå±€éƒ¨æ´ªæ°´å›¾åƒ\")\n",
    "    \n",
    "    # --- 4.åˆ›å»ºå…¨å±€çŸ©å½¢åŒºåŸŸæ·¹æ²¡ ---\n",
    "    print(f\"\\næ­¥éª¤4ï¼šåˆ›å»ºå…¨å±€çŸ©å½¢åŒºåŸŸçš„æ´ªæ°´æ·¹æ²¡\")\n",
    "    \n",
    "    # å°†æ‰€æœ‰çŸ©å½¢å‡ ä½•ä½“åˆå¹¶ä¸ºä¸€ä¸ªFeatureCollection\n",
    "    \n",
    "    rectangles_fc = ee.FeatureCollection([\n",
    "        ee.Feature(geom) for geom in rectangle_geometries\n",
    "    ])\n",
    "    \n",
    "    # åˆ›å»ºä¸€ä¸ªè¡¨ç¤ºæ‰€æœ‰çŸ©å½¢åŒºåŸŸçš„æ©è†œ\n",
    "    # åœ¨çŸ©å½¢å†…éƒ¨å€¼ä¸º1ï¼Œå¤–éƒ¨å€¼ä¸º0\n",
    "    rectangles_mask = ee.Image().byte().paint(rectangles_fc, 1).clip(roi_choose1)\n",
    "    \n",
    "    print(f\"  âœ“ åˆ›å»º {len(rectangle_geometries)} ä¸ªçŸ©å½¢çš„åˆå¹¶æ©è†œ\")\n",
    "    \n",
    "    # --- 5. å¯¹çŸ©å½¢å¤–éƒ¨åŒºåŸŸåº”ç”¨å…¨å±€å¹³å‡é˜ˆå€¼ ---\n",
    "    print(f\"\\næ­¥éª¤5: å¯¹çŸ©å½¢å¤–éƒ¨åŒºåŸŸåº”ç”¨å…¨å±€å¹³å‡é˜ˆå€¼\")\n",
    "    \n",
    "    # åœ¨æ•´ä¸ªç ”ç©¶åŒºåŸŸåº”ç”¨å…¨å±€é˜ˆå€¼\n",
    "    global_flood_mask = flood_image.lt(global_average_threshold)\n",
    "    \n",
    "    # åªä¿ç•™çŸ©å½¢å¤–éƒ¨çš„å…¨å±€æ´ªæ°´ï¼ˆçŸ©å½¢å†…éƒ¨è®¾ä¸º0ï¼‰\n",
    "    # çŸ©å½¢å¤–éƒ¨ï¼šrectangles_mask = 0ï¼ŒçŸ©å½¢å†…éƒ¨ï¼šrectangles_mask = 1\n",
    "    global_flood_outside = global_flood_mask.where(rectangles_mask.eq(1), 0)\n",
    "    \n",
    "    print(f\"  âœ“ å…¨å±€é˜ˆå€¼ {global_average_threshold:.2f} dB åº”ç”¨äºçŸ©å½¢å¤–éƒ¨åŒºåŸŸ\")\n",
    "    \n",
    "    # --- 6. åˆå¹¶å±€éƒ¨å’Œå…¨å±€æ´ªæ°´å›¾ ---\n",
    "    print(f\"\\næ­¥éª¤6: åˆå¹¶å±€éƒ¨å’Œå…¨å±€æ´ªæ°´å›¾\")\n",
    "    \n",
    "    # å°†å±€éƒ¨æ´ªæ°´å›¾çš„æ©è†œç§»é™¤ï¼Œç”¨0å¡«å……æœªæ©è†œåŒºåŸŸ\n",
    "    all_local_floods_unmasked = all_local_floods.unmask(0)\n",
    "    \n",
    "    # å°†å±€éƒ¨æ´ªæ°´ï¼ˆçŸ©å½¢å†…ï¼‰å’Œå…¨å±€æ´ªæ°´ï¼ˆçŸ©å½¢å¤–ï¼‰ç›¸åŠ \n",
    "    global_flood_outside_unmasked = global_flood_outside.unmask(0) # åŒæ ·ç”¨0å¡«å……æœªæ©è†œåŒºåŸŸ\n",
    "    final_flood_map = all_local_floods_unmasked.add(global_flood_outside_unmasked)\n",
    "    \n",
    "    # å°†ç»“æœé™åˆ¶åœ¨ç ”ç©¶åŒºåŸŸå†…\n",
    "    final_flood_map = final_flood_map.clip(roi_choose1).selfMask() # selfMaskï¼ˆï¼‰å°†åƒç´ å€¼ä¸º0çš„åŒºåŸŸè®¾ä¸ºé€æ˜\n",
    "    \n",
    "    print(f\"  âœ“ æˆåŠŸåˆå¹¶å±€éƒ¨æ´ªæ°´å›¾å’Œå…¨å±€æ´ªæ°´å›¾\")\n",
    "    \n",
    "    # --- 7. åˆ›å»ºæå–ä¿¡æ¯ ä¹Ÿæ˜¯ä¸€ä¸ªå­—å…¸ç±»å‹ ---\n",
    "    extraction_info = {\n",
    "        'num_rectangles': len(filtered_threshold_results), # int\n",
    "        'local_thresholds': local_thresholds, # list of float\n",
    "        'global_average_threshold': global_average_threshold, # float\n",
    "        'threshold_range': [min(local_thresholds), max(local_thresholds)], # list of float\n",
    "        'extraction_strategy': 'Adaptive: Local thresholds inside rectangles, Global average outside' # str\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ… è‡ªé€‚åº”æ´ªæ°´æå–å®Œæˆï¼\")\n",
    "    print(f\"  ç­–ç•¥: çŸ©å½¢å†…éƒ¨ä½¿ç”¨å±€éƒ¨é˜ˆå€¼ï¼Œå¤–éƒ¨ä½¿ç”¨å…¨å±€å¹³å‡é˜ˆå€¼\")\n",
    "    print(f\"  çŸ©å½¢æ•°é‡: {extraction_info['num_rectangles']}\")\n",
    "    print(f\"  å…¨å±€é˜ˆå€¼: {extraction_info['global_average_threshold']:.2f} dB\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return final_flood_map, extraction_info\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646083e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æœ€ç»ˆçš„æ´ªæ°´æå–ç»“æœå‡½æ•°\n",
    "def visualize_final_flood_results(final_flood_map, extraction_info, filtered_threshold_results, roi_choose1):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–æœ€ç»ˆçš„æ´ªæ°´æå–ç»“æœ\n",
    "    Args:\n",
    "        final_flood_map:æœ€ç»ˆæ´ªæ°´å›¾(ee.Image)\n",
    "        extraction_info:æå–ä¿¡æ¯(dict)\n",
    "        filtered_threshold_results(dict):è¿‡æ»¤åçš„é˜ˆå€¼ç»“æœåµŒå¥—å­—å…¸\n",
    "        roi_choose1:ç ”ç©¶åŒºåŸŸ(ee.Geometry)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- å¯è§†åŒ–æœ€ç»ˆæ´ªæ°´æå–ç»“æœ\")\n",
    "    Map_final = geemap.Map()\n",
    "    Map_final.centerObject(roi_choose1, 11)\n",
    "    \n",
    "    # æ·»åŠ åŸå§‹SARå½±åƒä½œä¸ºåº•å›¾\n",
    "    Map_final.addLayer(flood_image,{'min':-25,'max':5},'SAR Image(VV)',False)\n",
    "    flood_vis = {\n",
    "        'palette':['#0066CC'], # æ·±è“è‰²,éœ€è¦ä½¿ç”¨åˆ—è¡¨è€Œä¸æ˜¯é›†åˆ\n",
    "        'opacity':0.8\n",
    "    }\n",
    "    \n",
    "    # æ·»åŠ æ´ªæ°´ç»“æœå½±åƒ\n",
    "    Map_final.addLayer(final_flood_map, flood_vis, 'Final Flood Map', True)\n",
    "    \n",
    "    # æ·»åŠ çŸ©å½¢è¾¹ç•Œä»¥æ˜¾ç¤ºå±€éƒ¨é˜ˆå€¼åº”ç”¨åŒºåŸŸ\n",
    "    #  é¦–å…ˆæ”¶é›†çŸ©å½¢\n",
    "    rectangle_features = []\n",
    "    for result in filtered_threshold_results:\n",
    "        original_data = result['original_profile_data']\n",
    "        rectangle_feature = original_data['feature']\n",
    "        rectangle_features.append(rectangle_feature)\n",
    "    \n",
    "    # ç„¶åè½¬ä¸ºFeatureCollectionï¼Œå¹¶é€šè¿‡paintæ–¹æ³•ç»˜åˆ¶è¾¹ç•Œ\n",
    "    if rectangle_features:\n",
    "        rectangles_fc = ee.FeatureCollection(rectangle_features)\n",
    "        rectangle_outline = ee.Image().byte().paint(rectangles_fc,0,2)\n",
    "        Map_final.addLayer(rectangle_outline,{\n",
    "            'palette':'FFFF00' # é»„è‰²\n",
    "        }, 'Rectangles Outlines', True)\n",
    "        print(f\"  âœ“ æ˜¾ç¤º {len(rectangle_features)} ä¸ªå±€éƒ¨é˜ˆå€¼åº”ç”¨åŒºåŸŸï¼ˆé»„è‰²è¾¹æ¡†ï¼‰\")\n",
    "    \n",
    "    # æ·»åŠ æ¢¯åº¦æå€¼ç‚¹\n",
    "    neg_grad_points = []\n",
    "    pos_grad_points = []\n",
    "    \n",
    "    for result in filtered_threshold_results:\n",
    "        threshold_calc = result['threshold_calculation']\n",
    "        \n",
    "        neg_coords = threshold_calc.get('neg_grad_point_coords')\n",
    "        if neg_coords:\n",
    "            neg_grad_points.append(ee.Feature(ee.Geometry.Point(neg_coords)))\n",
    "            \n",
    "        pos_coords = threshold_calc.get('pos_grad_point_coords')\n",
    "        if pos_coords:\n",
    "            pos_grad_points.append(ee.Feature(ee.Geometry.Point(pos_coords)))\n",
    "    \n",
    "    if neg_grad_points:\n",
    "        neg_points_fc = ee.FeatureCollection(neg_grad_points)\n",
    "        Map_final.addLayer(neg_points_fc, {\n",
    "            'color': 'FF0000',\n",
    "            'pointSize': 6,\n",
    "            'pointShape': 'circle'\n",
    "        }, 'Negative Gradient Points')\n",
    "        print(f\"  âœ“ æ˜¾ç¤º {len(neg_grad_points)} ä¸ªè´Ÿæ¢¯åº¦æå€¼ç‚¹ï¼ˆçº¢è‰²ï¼‰\")\n",
    "    \n",
    "    if pos_grad_points:\n",
    "        pos_points_fc = ee.FeatureCollection(pos_grad_points)\n",
    "        Map_final.addLayer(pos_points_fc, {\n",
    "            'color': '0000FF',\n",
    "            'pointSize': 6,\n",
    "            'pointShape': 'circle'\n",
    "        }, 'Positive Gradient Points')\n",
    "        print(f\"  âœ“ æ˜¾ç¤º {len(pos_grad_points)} ä¸ªæ­£æ¢¯åº¦æå€¼ç‚¹ï¼ˆè“è‰²ï¼‰\")\n",
    "    \n",
    "    # æ‰“å°æå–ç»Ÿè®¡ä¿¡æ¯\n",
    "    print(f\"\\nğŸ“Š æœ€ç»ˆæ´ªæ°´æå–ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "    print(f\"  æå–ç­–ç•¥: {extraction_info['extraction_strategy']}\")\n",
    "    print(f\"  å±€éƒ¨é˜ˆå€¼çŸ©å½¢æ•°é‡: {extraction_info['num_rectangles']}\")\n",
    "    print(f\"  å…¨å±€å¹³å‡é˜ˆå€¼: {extraction_info['global_average_threshold']:.2f} dB\")\n",
    "    print(f\"  å±€éƒ¨é˜ˆå€¼èŒƒå›´: [{extraction_info['threshold_range'][0]:.2f}, {extraction_info['threshold_range'][1]:.2f}] dB\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå›¾ä¾‹è¯´æ˜\n",
    "    print(f\"\\nğŸ—ºï¸ åœ°å›¾å›¾å±‚è¯´æ˜:\")\n",
    "    print(f\"  ğŸ”µ æ·±è“è‰²åŒºåŸŸ: æœ€ç»ˆæ´ªæ°´æ·¹æ²¡èŒƒå›´\")\n",
    "    print(f\"  ğŸŸ¨ é»„è‰²è¾¹æ¡†: åº”ç”¨å±€éƒ¨é˜ˆå€¼çš„çŸ©å½¢åŒºåŸŸ\")\n",
    "    print(f\"  ğŸ”´ çº¢è‰²ç‚¹: è´Ÿæ¢¯åº¦æå€¼ç‚¹\")\n",
    "    print(f\"  ğŸ”µ è“è‰²ç‚¹: æ­£æ¢¯åº¦æå€¼ç‚¹\")\n",
    "    print(f\"  ğŸ“ çŸ©å½¢å†…: ä½¿ç”¨å„è‡ªçš„å±€éƒ¨é˜ˆå€¼\")\n",
    "    print(f\"  ğŸ“ çŸ©å½¢å¤–: ä½¿ç”¨å…¨å±€å¹³å‡é˜ˆå€¼ {extraction_info['global_average_threshold']:.2f} dB\")\n",
    "    \n",
    "    return Map_final\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5db7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸»æ‰§è¡Œæµç¨‹ï¼šè‡ªé€‚åº”æ´ªæ°´æå–\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# ç¬¬8èŠ‚ï¼šç»“åˆå±€éƒ¨æœ€ä¼˜å’Œå…¨å±€æœ€ä¼˜çš„æ´ªæ°´æå–\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "# æ£€æŸ¥å‰ææ¡ä»¶\n",
    "if 'local_threshold_results' not in globals() or not local_threshold_results:\n",
    "    print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° 'local_threshold_results' æ•°æ®\")\n",
    "    print(\"   è¯·ç¡®ä¿æ‚¨å·²æˆåŠŸè¿è¡Œäº†ç¬¬7.4èŠ‚çš„å±€éƒ¨é˜ˆå€¼è®¡ç®—ä»£ç \")\n",
    "else:\n",
    "    print(f\"âœ“ æ‰¾åˆ° {len(local_threshold_results)} ä¸ªå±€éƒ¨é˜ˆå€¼ç»“æœ\")\n",
    "    \n",
    "    # æ­¥éª¤1: è¿‡æ»¤é˜ˆå€¼å¤§äº-8çš„çŸ©å½¢\n",
    "    filtered_results, filter_statistics = filter_threshold_results(\n",
    "        local_threshold_results, \n",
    "        threshold_limit=-8\n",
    "    )\n",
    "    \n",
    "    if not filtered_results:\n",
    "        print(\"âŒ æ‰€æœ‰çŸ©å½¢éƒ½è¢«è¿‡æ»¤æ‰äº†ï¼Œæ— æ³•è¿›è¡Œæ´ªæ°´æå–\")\n",
    "        print(\"   å»ºè®®ï¼šè°ƒæ•´é˜ˆå€¼è¿‡æ»¤æ¡ä»¶æˆ–æ£€æŸ¥å±€éƒ¨é˜ˆå€¼è®¡ç®—ç»“æœ\")\n",
    "    else:\n",
    "        # æ­¥éª¤2: æ‰§è¡Œè‡ªé€‚åº”æ´ªæ°´æå–\n",
    "        final_flood_map, extraction_info = create_adaptive_flood_extraction(\n",
    "            filtered_results,\n",
    "            flood_image,\n",
    "            roi_choose1\n",
    "        )\n",
    "        \n",
    "        if final_flood_map is not None:\n",
    "            # æ­¥éª¤3: å¯è§†åŒ–æœ€ç»ˆç»“æœ\n",
    "            Map_final_flood = visualize_final_flood_results(\n",
    "                final_flood_map,\n",
    "                extraction_info,\n",
    "                filtered_results,\n",
    "                roi_choose1\n",
    "            )\n",
    "            \n",
    "            # ä¿å­˜æœ€ç»ˆç»“æœä¸ºå…¨å±€å˜é‡\n",
    "            global final_flood_extraction_results\n",
    "            final_flood_extraction_results = {\n",
    "                'flood_map': final_flood_map,\n",
    "                'extraction_info': extraction_info,\n",
    "                'filtered_results': filtered_results,\n",
    "                'filter_statistics': filter_statistics\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nğŸ‰ è‡ªé€‚åº”æ´ªæ°´æå–æˆåŠŸå®Œæˆï¼\")\n",
    "            print(f\"   âœ“ æœ€ç»ˆæ´ªæ°´å›¾å·²ç”Ÿæˆ\")\n",
    "            print(f\"   âœ“ ç»“æœå·²ä¿å­˜åˆ°å…¨å±€å˜é‡ 'final_flood_extraction_results'\")\n",
    "            print(f\"   âœ“ åœ°å›¾å·²æ›´æ–°å¹¶å¯è§†åŒ–\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ æ´ªæ°´æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# è‡ªé€‚åº”æ´ªæ°´æå–æ¨¡å—æ‰§è¡Œå®Œæ¯•\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "# æ˜¾ç¤ºæœ€ç»ˆç»“æœåœ°å›¾\n",
    "Map_final_flood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56da02",
   "metadata": {},
   "source": [
    "# 9.ç»“åˆå±€éƒ¨åˆ†å—çš„æ´ªæ°´æå–ç»“æœ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0d7b4",
   "metadata": {},
   "source": [
    "## 9.1 æ··åˆæ ¼ç½‘åˆ†å—ç­–ç•¥è‡ªé€‚åº”åˆ†å‰²å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93437e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def create_hybrid_grid_segmentation(filtered_threshold_results, flood_image, roi_choose1):\n",
    "    \"\"\"\n",
    "    å®ç°æ··åˆæ ¼ç½‘åˆ†å—ç­–ç•¥çš„è‡ªé€‚åº”åˆ†å‰²\n",
    "    \n",
    "    Args:\n",
    "        filtered_threshold_results: è¿‡æ»¤åçš„å±€éƒ¨é˜ˆå€¼ç»“æœ\n",
    "        flood_image: SARå½±åƒ\n",
    "        roi_choose1: ç ”ç©¶åŒºåŸŸ\n",
    "    \n",
    "    Returns:\n",
    "        final_segmentation_map: æœ€ç»ˆåˆ†å‰²ç»“æœ\n",
    "        segmentation_info: åˆ†å‰²ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n=== ç¬¬9ç« ï¼šæ··åˆæ ¼ç½‘åˆ†å—ç­–ç•¥å®ç° ===\")\n",
    "    \n",
    "    # æ£€æŸ¥å‰ææ¡ä»¶\n",
    "    if not filtered_threshold_results:\n",
    "        print(\"âŒ é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„å±€éƒ¨é˜ˆå€¼ç»“æœ\")\n",
    "        return None, None\n",
    "    \n",
    "    # === æ­¥éª¤1: è·å–å›¾åƒå°ºå¯¸å¹¶è®¡ç®—åŸºæœ¬åˆ†å—å°ºåº¦B ===\n",
    "    print(\"\\næ­¥éª¤1: è®¡ç®—åŸºæœ¬åˆ†å—å°ºåº¦B\")\n",
    "    \n",
    "    # è·å–ç ”ç©¶åŒºåŸŸçš„è¾¹ç•Œä¿¡æ¯\n",
    "    roi_bounds = roi_choose1.bounds().getInfo()\n",
    "    west = roi_bounds['coordinates'][0][0][0]\n",
    "    south = roi_bounds['coordinates'][0][0][1] \n",
    "    east = roi_bounds['coordinates'][0][2][0]\n",
    "    north = roi_bounds['coordinates'][0][2][1]\n",
    "    \n",
    "    # è®¡ç®—å›¾åƒçš„åƒç´ å°ºå¯¸ï¼ˆå‡è®¾10ç±³åˆ†è¾¨ç‡ï¼‰\n",
    "    pixel_size = 10  # ç±³/åƒç´ \n",
    "    W = int((east - west) * 111320 * math.cos(math.radians((north + south) / 2)) / pixel_size)  # åˆ—æ•°\n",
    "    H = int((north - south) * 111320 / pixel_size)  # è¡Œæ•°\n",
    "    \n",
    "    # è®¡ç®—åŸºæœ¬åˆ†å—å°ºåº¦B = min(H,W)/4\n",
    "    B = min(H, W) // 4\n",
    "    \n",
    "    print(f\"  âœ“ å›¾åƒå°ºå¯¸: {H} è¡Œ Ã— {W} åˆ—\")\n",
    "    print(f\"  âœ“ åŸºæœ¬åˆ†å—å°ºåº¦ B = min({H}, {W})/4 = {B} åƒç´ \")\n",
    "    print(f\"  âœ“ å¯¹åº”å®é™…å°ºå¯¸: {B * pixel_size} ç±³\")\n",
    "    \n",
    "    # === æ­¥éª¤2: ä¸è§„åˆ™å…³é”®åŒºåŸŸæå– ===\n",
    "    print(\"\\næ­¥éª¤2: æå–ä¸è§„åˆ™å…³é”®åŒºåŸŸï¼ˆå¤§å°ºå¯¸å€™é€‰çŸ©å½¢ï¼‰\")\n",
    "    \n",
    "    large_rectangles = []  # å­˜å‚¨å¤§å°ºå¯¸çŸ©å½¢\n",
    "    large_rectangle_thresholds = {}  # å­˜å‚¨å¤§çŸ©å½¢å¯¹åº”çš„é˜ˆå€¼\n",
    "    \n",
    "    for result in filtered_threshold_results:\n",
    "        # è·å–çŸ©å½¢çš„å®é™…å°ºå¯¸ï¼ˆç±³ï¼‰\n",
    "        original_data = result['original_profile_data']\n",
    "        width_meters = original_data['width_meters']\n",
    "        height_meters = original_data['height_meters']\n",
    "        \n",
    "        # è½¬æ¢ä¸ºåƒç´ å°ºå¯¸\n",
    "        width_pixels = width_meters / pixel_size\n",
    "        height_pixels = height_meters / pixel_size\n",
    "        \n",
    "        # åˆ¤æ–­æ˜¯å¦ä¸ºå¤§å°ºå¯¸çŸ©å½¢ï¼ˆä»»æ„è¾¹é•¿ > Bï¼‰\n",
    "        if max(width_pixels, height_pixels) > B:\n",
    "            rectangle_feature = original_data['feature']\n",
    "            local_threshold = result['threshold_calculation']['local_threshold']\n",
    "            \n",
    "            large_rectangles.append(rectangle_feature)\n",
    "            large_rectangle_thresholds[str(rectangle_feature.geometry().getInfo())] = local_threshold\n",
    "            \n",
    "            print(f\"  âœ“ å‘ç°å¤§çŸ©å½¢: {width_pixels:.1f}Ã—{height_pixels:.1f} åƒç´ , é˜ˆå€¼: {local_threshold:.2f} dB\")\n",
    "    \n",
    "    print(f\"  æ€»è®¡å‘ç° {len(large_rectangles)} ä¸ªå¤§å°ºå¯¸çŸ©å½¢ä½œä¸ºä¸è§„åˆ™åˆ†å—\")\n",
    "    \n",
    "    # === æ­¥éª¤3: è§„åˆ™æ ¼ç½‘åˆ’åˆ† ===\n",
    "    print(\"\\næ­¥éª¤3: å¯¹å‰©ä½™åŒºåŸŸè¿›è¡Œè§„åˆ™æ ¼ç½‘åˆ’åˆ†\")\n",
    "    \n",
    "    # è®¡ç®—ç†è®ºåˆ†å—æ•°\n",
    "    r = round(W / B)  # åˆ—æ–¹å‘åˆ†å—æ•°\n",
    "    l = round(H / B)  # è¡Œæ–¹å‘åˆ†å—æ•°\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªæ ‡å‡†æ ¼ç½‘çš„å°ºå¯¸\n",
    "    w = round(W / r)  # æ ‡å‡†æ ¼ç½‘å®½åº¦\n",
    "    h = round(H / l)  # æ ‡å‡†æ ¼ç½‘é«˜åº¦\n",
    "    \n",
    "    print(f\"  âœ“ ç†è®ºåˆ†å—æ•°: è¡Œæ–¹å‘{l}å—, åˆ—æ–¹å‘{r}å—\")\n",
    "    print(f\"  âœ“ æ ‡å‡†æ ¼ç½‘å°ºå¯¸: {h}è¡Œ Ã— {w}åˆ— ({h*pixel_size}m Ã— {w*pixel_size}m)\")\n",
    "    \n",
    "    # åˆ›å»ºè§„åˆ™æ ¼ç½‘\n",
    "    grid_cells = []\n",
    "    for i in range(l):\n",
    "        for j in range(r):\n",
    "            # è®¡ç®—å½“å‰æ ¼ç½‘çš„åƒç´ åæ ‡èŒƒå›´\n",
    "            start_row = i * h\n",
    "            end_row = (i + 1) * h if i < l - 1 else H  # æœ€åä¸€è¡Œç‰¹æ®Šå¤„ç†\n",
    "            start_col = j * w  \n",
    "            end_col = (j + 1) * w if j < r - 1 else W  # æœ€åä¸€åˆ—ç‰¹æ®Šå¤„ç†\n",
    "            \n",
    "            # è½¬æ¢ä¸ºåœ°ç†åæ ‡\n",
    "            cell_west = west + (start_col * pixel_size) / (111320 * math.cos(math.radians((north + south) / 2)))\n",
    "            cell_east = west + (end_col * pixel_size) / (111320 * math.cos(math.radians((north + south) / 2)))\n",
    "            cell_south = south + (start_row * pixel_size) / 111320\n",
    "            cell_north = south + (end_row * pixel_size) / 111320\n",
    "            \n",
    "            # åˆ›å»ºæ ¼ç½‘å‡ ä½•ä½“\n",
    "            cell_geometry = ee.Geometry.Rectangle([cell_west, cell_south, cell_east, cell_north])\n",
    "            \n",
    "            grid_cells.append({\n",
    "                'geometry': cell_geometry,\n",
    "                'id': f'grid_{i}_{j}',\n",
    "                'center_lat': (cell_north + cell_south) / 2,\n",
    "                'center_lon': (cell_east + cell_west) / 2,\n",
    "                'pixel_size': (end_row - start_row, end_col - start_col)\n",
    "            })\n",
    "    \n",
    "    print(f\"  âœ“ åˆ›å»ºäº† {len(grid_cells)} ä¸ªè§„åˆ™æ ¼ç½‘å•å…ƒ\")\n",
    "    \n",
    "    # === æ­¥éª¤4: åˆ†å—é˜ˆå€¼åˆ†é… ===\n",
    "    print(\"\\næ­¥éª¤4: ä¸ºæ¯ä¸ªåˆ†å—åˆ†é…æœ€ä¼˜é˜ˆå€¼\")\n",
    "    \n",
    "    # å‡†å¤‡å€™é€‰çŸ©å½¢çš„ä¸­å¿ƒç‚¹å’Œé˜ˆå€¼ä¿¡æ¯\n",
    "    rectangle_centers = []\n",
    "    rectangle_thresholds = []\n",
    "    rectangle_areas = []\n",
    "    \n",
    "    for result in filtered_threshold_results:\n",
    "        original_data = result['original_profile_data']\n",
    "        feature = original_data['feature']\n",
    "        threshold = result['threshold_calculation']['local_threshold']\n",
    "        \n",
    "        # è®¡ç®—çŸ©å½¢ä¸­å¿ƒç‚¹\n",
    "        center = feature.geometry().centroid(ee.ErrorMargin(1)).coordinates().getInfo()\n",
    "        center_lon, center_lat = center[0], center[1]\n",
    "        \n",
    "        # è®¡ç®—çŸ©å½¢é¢ç§¯\n",
    "        area = original_data['width_meters'] * original_data['height_meters']\n",
    "        \n",
    "        rectangle_centers.append((center_lon, center_lat))\n",
    "        rectangle_thresholds.append(threshold)\n",
    "        rectangle_areas.append(area)\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªæ ¼ç½‘åˆ†é…é˜ˆå€¼\n",
    "    grid_thresholds = {}\n",
    "    assignment_stats = {'inside': 0, 'nearest': 0}\n",
    "    \n",
    "    for cell in grid_cells:\n",
    "        cell_center_lon = cell['center_lon']\n",
    "        cell_center_lat = cell['center_lat']\n",
    "        cell_geom = cell['geometry']\n",
    "        \n",
    "        # æ–¹æ³•1: æ£€æŸ¥æ ¼ç½‘å†…æ˜¯å¦åŒ…å«å€™é€‰çŸ©å½¢ä¸­å¿ƒç‚¹\n",
    "        contained_rectangles = []\n",
    "        for idx, (rect_lon, rect_lat) in enumerate(rectangle_centers):\n",
    "            rect_point = ee.Geometry.Point([rect_lon, rect_lat])\n",
    "            if cell_geom.contains(rect_point).getInfo():\n",
    "                contained_rectangles.append({\n",
    "                    'index': idx,\n",
    "                    'area': rectangle_areas[idx],\n",
    "                    'threshold': rectangle_thresholds[idx]\n",
    "                })\n",
    "        \n",
    "        if contained_rectangles:\n",
    "            # é€‰æ‹©é¢ç§¯æœ€å¤§çš„çŸ©å½¢çš„é˜ˆå€¼\n",
    "            largest_rect = max(contained_rectangles, key=lambda x: x['area'])\n",
    "            assigned_threshold = largest_rect['threshold']\n",
    "            assignment_stats['inside'] += 1\n",
    "            assignment_method = 'inside'\n",
    "        else:\n",
    "            # æ–¹æ³•2: ä½¿ç”¨æœ€è¿‘é‚»åŸåˆ™\n",
    "            min_distance = float('inf')\n",
    "            nearest_threshold = None\n",
    "            \n",
    "            for idx, (rect_lon, rect_lat) in enumerate(rectangle_centers):\n",
    "                # è®¡ç®—æ¬§æ°è·ç¦»\n",
    "                distance = math.sqrt((cell_center_lon - rect_lon)**2 + (cell_center_lat - rect_lat)**2)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    nearest_threshold = rectangle_thresholds[idx]\n",
    "            \n",
    "            assigned_threshold = nearest_threshold\n",
    "            assignment_stats['nearest'] += 1\n",
    "            assignment_method = 'nearest'\n",
    "        \n",
    "        grid_thresholds[cell['id']] = {\n",
    "            'threshold': assigned_threshold,\n",
    "            'method': assignment_method,\n",
    "            'geometry': cell_geom\n",
    "        }\n",
    "    \n",
    "    print(f\"  âœ“ é˜ˆå€¼åˆ†é…å®Œæˆ:\")\n",
    "    print(f\"    - {assignment_stats['inside']} ä¸ªæ ¼ç½‘ä½¿ç”¨å†…éƒ¨çŸ©å½¢é˜ˆå€¼\")\n",
    "    print(f\"    - {assignment_stats['nearest']} ä¸ªæ ¼ç½‘ä½¿ç”¨æœ€è¿‘é‚»é˜ˆå€¼\")\n",
    "    \n",
    "    # === æ­¥éª¤5: æ‰§è¡Œåˆ†å—åˆ†å‰² ===\n",
    "    print(\"\\næ­¥éª¤5: æ‰§è¡Œåˆ†å—åˆ†å‰²å¹¶é•¶åµŒç»“æœ\")\n",
    "    \n",
    "    segmented_blocks = []\n",
    "    \n",
    "    # å¤„ç†å¤§å°ºå¯¸çŸ©å½¢ï¼ˆä¸è§„åˆ™åˆ†å—ï¼‰\n",
    "    for rect_feature in large_rectangles:\n",
    "        rect_geom_str = str(rect_feature.geometry().getInfo())\n",
    "        threshold = large_rectangle_thresholds[rect_geom_str]\n",
    "        \n",
    "        # åœ¨è¯¥çŸ©å½¢åŒºåŸŸåº”ç”¨å±€éƒ¨é˜ˆå€¼\n",
    "        local_mask = flood_image.lt(threshold)\n",
    "        clipped_result = local_mask.clip(rect_feature.geometry()).selfMask()\n",
    "        segmented_blocks.append(clipped_result)\n",
    "    \n",
    "    # å¤„ç†è§„åˆ™æ ¼ç½‘\n",
    "    for cell_id, threshold_info in grid_thresholds.items():\n",
    "        threshold = threshold_info['threshold']\n",
    "        cell_geom = threshold_info['geometry']\n",
    "        \n",
    "        # åœ¨è¯¥æ ¼ç½‘åŒºåŸŸåº”ç”¨åˆ†é…çš„é˜ˆå€¼\n",
    "        local_mask = flood_image.lt(threshold)\n",
    "        clipped_result = local_mask.clip(cell_geom).selfMask()\n",
    "        segmented_blocks.append(clipped_result)\n",
    "    \n",
    "    # é•¶åµŒæ‰€æœ‰åˆ†å‰²ç»“æœ\n",
    "    if len(segmented_blocks) == 1:\n",
    "        final_segmentation_map = segmented_blocks[0]\n",
    "    else:\n",
    "        segmentation_collection = ee.ImageCollection(segmented_blocks)\n",
    "        final_segmentation_map = segmentation_collection.mosaic().clip(roi_choose1)\n",
    "    \n",
    "    print(f\"  âœ“ å·²é•¶åµŒ {len(segmented_blocks)} ä¸ªåˆ†å—çš„åˆ†å‰²ç»“æœ\")\n",
    "    \n",
    "    # === åˆ›å»ºåˆ†å‰²ä¿¡æ¯ ===\n",
    "    segmentation_info = {\n",
    "        'method': 'Hybrid Grid Block Segmentation',\n",
    "        'image_size': (H, W),\n",
    "        'block_scale_B': B,\n",
    "        'grid_size': (l, r),\n",
    "        'standard_cell_size': (h, w),\n",
    "        'total_large_rectangles': len(large_rectangles),\n",
    "        'total_grid_cells': len(grid_cells),\n",
    "        'threshold_assignment': assignment_stats,\n",
    "        'pixel_size_meters': pixel_size\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ… æ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²å®Œæˆï¼\")\n",
    "    print(f\"  æ€»åˆ†å—æ•°: {len(large_rectangles) + len(grid_cells)}\")\n",
    "    print(f\"  - ä¸è§„åˆ™åˆ†å—: {len(large_rectangles)} ä¸ª\")\n",
    "    print(f\"  - è§„åˆ™æ ¼ç½‘: {len(grid_cells)} ä¸ª\")\n",
    "    \n",
    "    return final_segmentation_map, segmentation_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e5151",
   "metadata": {},
   "source": [
    "## 9.2 å¯è§†åŒ–ç»“æœå‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 ä¿®æ”¹ç‰ˆå¯è§†åŒ–ç»“æœå‡½æ•° - æ·»åŠ åˆ°ç°æœ‰åœ°å›¾\n",
    "def add_hybrid_grid_to_existing_map(final_segmentation_map, segmentation_info, filtered_threshold_results, existing_map):\n",
    "    \"\"\"\n",
    "    å°†æ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²ç»“æœæ·»åŠ åˆ°ç°æœ‰çš„åœ°å›¾ä¸Š\n",
    "    Args:\n",
    "        final_segmentation_map: æ··åˆæ ¼ç½‘åˆ†å‰²ç»“æœå½±åƒ\n",
    "        segmentation_info: åˆ†å‰²ä¿¡æ¯å­—å…¸\n",
    "        filtered_threshold_results: è¿‡æ»¤åçš„é˜ˆå€¼ç»“æœ\n",
    "        existing_map: ç°æœ‰çš„geemapåœ°å›¾å¯¹è±¡ï¼ˆç¬¬å…«ç« çš„Map_final_floodï¼‰\n",
    "    \"\"\"\n",
    "    print(\"\\n--- å°†æ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²ç»“æœæ·»åŠ åˆ°ç°æœ‰åœ°å›¾ ---\")\n",
    "    \n",
    "    # æ·»åŠ æ··åˆæ ¼ç½‘åˆ†å‰²ç»“æœï¼ˆä½¿ç”¨ä¸åŒé¢œè‰²ä»¥åŒºåˆ†ç¬¬å…«ç« ç»“æœï¼‰\n",
    "    grid_segmentation_vis = {\n",
    "        'palette': ['#FF6B6B'],  # çº¢è‰²ç³»\n",
    "        'opacity': 0.6\n",
    "    }\n",
    "    existing_map.addLayer(final_segmentation_map, grid_segmentation_vis, 'Method 2: Hybrid Grid Segmentation', True)\n",
    "    \n",
    "    # æ·»åŠ æ ¼ç½‘åˆ†å—çš„å€™é€‰çŸ©å½¢è¾¹ç•Œï¼ˆä½¿ç”¨ä¸åŒé¢œè‰²ï¼‰\n",
    "    rectangle_features = []\n",
    "    for result in filtered_threshold_results:\n",
    "        rectangle_features.append(result['original_profile_data']['feature'])\n",
    "    \n",
    "    if rectangle_features:\n",
    "        rectangles_fc = ee.FeatureCollection(rectangle_features)\n",
    "        rectangle_outline = ee.Image().byte().paint(rectangles_fc, 0, 1)\n",
    "        existing_map.addLayer(rectangle_outline, {'palette': '00FFFF'}, 'Grid Method Rectangles', False)  # é»˜è®¤å…³é—­ï¼Œé¿å…ä¸ç¬¬å…«ç« é‡å \n",
    "    \n",
    "    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯\n",
    "    print(f\"\\nğŸ“Š æ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²ç»Ÿè®¡:\")\n",
    "    print(f\"  åˆ†å‰²æ–¹æ³•: {segmentation_info['method']}\")\n",
    "    print(f\"  å›¾åƒå°ºå¯¸: {segmentation_info['image_size'][0]} Ã— {segmentation_info['image_size'][1]} åƒç´ \")\n",
    "    print(f\"  åŸºæœ¬åˆ†å—å°ºåº¦: {segmentation_info['block_scale_B']} åƒç´ \")\n",
    "    print(f\"  è§„åˆ™æ ¼ç½‘: {segmentation_info['grid_size'][0]} Ã— {segmentation_info['grid_size'][1]}\")\n",
    "    print(f\"  ä¸è§„åˆ™åˆ†å—æ•°: {segmentation_info['total_large_rectangles']}\")\n",
    "    print(f\"  è§„åˆ™æ ¼ç½‘æ•°: {segmentation_info['total_grid_cells']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ—ºï¸ æ›´æ–°åçš„åœ°å›¾å›¾å±‚è¯´æ˜:\")\n",
    "    print(f\"  ğŸ”µ æ·±è“è‰²åŒºåŸŸ: æ–¹æ³•1-è‡ªé€‚åº”æ´ªæ°´æå–ç»“æœ\")\n",
    "    print(f\"  ğŸ”´ çº¢è‰²åŒºåŸŸ: æ–¹æ³•2-æ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²ç»“æœ\")\n",
    "    print(f\"  ğŸŸ¨ é»„è‰²è¾¹æ¡†: æ–¹æ³•1ä½¿ç”¨çš„çŸ©å½¢åŒºåŸŸ\")\n",
    "    print(f\"  ğŸ”µ é’è‰²è¾¹æ¡†: æ–¹æ³•2ä½¿ç”¨çš„çŸ©å½¢åŒºåŸŸï¼ˆé»˜è®¤éšè—ï¼‰\")\n",
    "    print(f\"  ğŸ”´ çº¢è‰²ç‚¹: è´Ÿæ¢¯åº¦æå€¼ç‚¹\")\n",
    "    print(f\"  ğŸ”µ è“è‰²ç‚¹: æ­£æ¢¯åº¦æå€¼ç‚¹\")\n",
    "    print(f\"  ğŸ“ ä¸¤ç§æ–¹æ³•åœ¨åŒä¸€åœ°å›¾ä¸Šè¿›è¡Œå¯¹æ¯”æ˜¾ç¤º\")\n",
    "    \n",
    "    return existing_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸»æ‰§è¡Œæµç¨‹ï¼šæ··åˆæ ¼ç½‘åˆ†å—ç­–ç•¥ï¼ˆä¿®æ”¹ç‰ˆ - ä½¿ç”¨ç°æœ‰åœ°å›¾ï¼‰\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# ç¬¬9èŠ‚æ‰§è¡Œï¼šæ··åˆæ ¼ç½‘åˆ†å—ç­–ç•¥çš„è‡ªé€‚åº”åˆ†å‰²ï¼ˆä½¿ç”¨ç¬¬8ç« åœ°å›¾ï¼‰\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "# æ£€æŸ¥å‰ææ¡ä»¶\n",
    "prerequisites_check = True\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰ç¬¬å…«ç« çš„åœ°å›¾ç»“æœ\n",
    "if 'Map_final_flood' not in globals():\n",
    "    print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ç¬¬å…«ç« çš„ 'Map_final_flood' åœ°å›¾\")\n",
    "    print(\"   è¯·ç¡®ä¿æ‚¨å·²æˆåŠŸè¿è¡Œäº†ç¬¬8èŠ‚çš„è‡ªé€‚åº”æ´ªæ°´æå–ä»£ç \")\n",
    "    prerequisites_check = False\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰å±€éƒ¨é˜ˆå€¼ç»“æœ\n",
    "if 'local_threshold_results' not in globals() or not local_threshold_results:\n",
    "    print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° 'local_threshold_results' æ•°æ®\")\n",
    "    print(\"   è¯·ç¡®ä¿æ‚¨å·²æˆåŠŸè¿è¡Œäº†ç¬¬7.4èŠ‚çš„å±€éƒ¨é˜ˆå€¼è®¡ç®—ä»£ç \")\n",
    "    prerequisites_check = False\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰æ´ªæ°´å½±åƒ\n",
    "if 'flood_image' not in globals() or flood_image is None:\n",
    "    print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° 'flood_image' æ•°æ®\")\n",
    "    print(\"   è¯·ç¡®ä¿æ‚¨å·²æˆåŠŸè¿è¡Œäº†ç¬¬4èŠ‚çš„å½±åƒåŠ è½½ä»£ç \")\n",
    "    prerequisites_check = False\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰ç ”ç©¶åŒºåŸŸ\n",
    "if 'roi_choose1' not in globals() or roi_choose1 is None:\n",
    "    print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° 'roi_choose1' æ•°æ®\")\n",
    "    print(\"   è¯·ç¡®ä¿æ‚¨å·²æˆåŠŸå®šä¹‰äº†ç ”ç©¶åŒºåŸŸ\")\n",
    "    prerequisites_check = False\n",
    "\n",
    "if prerequisites_check:\n",
    "    print(f\"âœ“ å‰ææ¡ä»¶æ£€æŸ¥é€šè¿‡\")\n",
    "    print(f\"âœ“ æ‰¾åˆ°ç¬¬å…«ç« åœ°å›¾å’Œ {len(local_threshold_results)} ä¸ªå±€éƒ¨é˜ˆå€¼ç»“æœ\")\n",
    "    \n",
    "    # === æ­¥éª¤0 - é˜ˆå€¼è¿‡æ»¤ï¼ˆé‡ç”¨ç¬¬å…«ç« çš„è¿‡æ»¤é€»è¾‘ï¼‰===\n",
    "    print(f\"\\næ­¥éª¤0: è¿‡æ»¤é˜ˆå€¼è¿‡å¤§çš„çŸ©å½¢ï¼ˆé‡ç”¨8.1çš„è¿‡æ»¤é€»è¾‘ï¼‰\")\n",
    "    \n",
    "    # è°ƒç”¨8.1çš„è¿‡æ»¤å‡½æ•°\n",
    "    filtered_results_for_grid, filter_stats_for_grid = filter_threshold_results(\n",
    "        local_threshold_results, \n",
    "        threshold_limit=-8\n",
    "    )\n",
    "    \n",
    "    if not filtered_results_for_grid:\n",
    "        print(\"âŒ æ‰€æœ‰çŸ©å½¢éƒ½è¢«è¿‡æ»¤æ‰äº†ï¼Œæ— æ³•è¿›è¡Œæ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²\")\n",
    "        print(\"   å»ºè®®ï¼šè°ƒæ•´é˜ˆå€¼è¿‡æ»¤æ¡ä»¶æˆ–æ£€æŸ¥å±€éƒ¨é˜ˆå€¼è®¡ç®—ç»“æœ\")\n",
    "        prerequisites_check = False\n",
    "    else:\n",
    "        print(f\"âœ“ è¿‡æ»¤åå‰©ä½™ {len(filtered_results_for_grid)} ä¸ªæœ‰æ•ˆçŸ©å½¢ç”¨äºæ ¼ç½‘åˆ†å—\")\n",
    "\n",
    "if prerequisites_check:\n",
    "    try:\n",
    "        # æ‰§è¡Œæ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²\n",
    "        hybrid_segmentation_map, hybrid_info = create_hybrid_grid_segmentation(\n",
    "            filtered_results_for_grid,\n",
    "            flood_image,\n",
    "            roi_choose1\n",
    "        )\n",
    "        \n",
    "        if hybrid_segmentation_map is not None:\n",
    "            # å°†ç»“æœæ·»åŠ åˆ°ç¬¬å…«ç« çš„ç°æœ‰åœ°å›¾ä¸Š\n",
    "            Map_final_flood_updated = add_hybrid_grid_to_existing_map(\n",
    "                hybrid_segmentation_map,\n",
    "                hybrid_info,\n",
    "                filtered_results_for_grid,\n",
    "                Map_final_flood  # ä½¿ç”¨ç¬¬å…«ç« çš„åœ°å›¾\n",
    "            )\n",
    "            \n",
    "            # ä¿å­˜ç»“æœä¸ºå…¨å±€å˜é‡\n",
    "            global hybrid_grid_results\n",
    "            hybrid_grid_results = {\n",
    "                'segmentation_map': hybrid_segmentation_map,\n",
    "                'segmentation_info': hybrid_info,\n",
    "                'filtered_results': filtered_results_for_grid,\n",
    "                'filter_statistics': filter_stats_for_grid,\n",
    "                'combined_map': Map_final_flood_updated  # ä¿å­˜æ›´æ–°åçš„ç»„åˆåœ°å›¾\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nğŸ‰ æ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²æˆåŠŸå®Œæˆå¹¶æ·»åŠ åˆ°ç¬¬å…«ç« åœ°å›¾ï¼\")\n",
    "            print(f\"   âœ“ ä½¿ç”¨äº† {len(filtered_results_for_grid)} ä¸ªè¿‡æ»¤åçš„æœ‰æ•ˆçŸ©å½¢\")\n",
    "            print(f\"   âœ“ åˆ†å‰²ç»“æœå·²ç”Ÿæˆå¹¶æ·»åŠ åˆ°ç°æœ‰åœ°å›¾\")\n",
    "            print(f\"   âœ“ ç»“æœå·²ä¿å­˜åˆ°å…¨å±€å˜é‡ 'hybrid_grid_results'\")\n",
    "            print(f\"   âœ“ ä¸¤ç§æ–¹æ³•ç°åœ¨å¯ä»¥åœ¨åŒä¸€åœ°å›¾ä¸Šå¯¹æ¯”æŸ¥çœ‹\")\n",
    "            \n",
    "            # æ˜¾ç¤ºè¿‡æ»¤ç»Ÿè®¡æ‘˜è¦\n",
    "            print(f\"\\nğŸ“Š é˜ˆå€¼è¿‡æ»¤æ‘˜è¦:\")\n",
    "            print(f\"   åŸå§‹çŸ©å½¢æ•°é‡: {len(local_threshold_results)}\")\n",
    "            print(f\"   è¿‡æ»¤åæ•°é‡: {filter_stats_for_grid['total_kept']}\")\n",
    "            print(f\"   ç§»é™¤æ•°é‡: {filter_stats_for_grid['total_removed']}\")\n",
    "            if filter_stats_for_grid['valid_thresholds']:\n",
    "                print(f\"   æœ‰æ•ˆé˜ˆå€¼èŒƒå›´: [{filter_stats_for_grid['min_threshold']:.2f}, {filter_stats_for_grid['max_threshold']:.2f}] dB\")\n",
    "            \n",
    "            # æ˜¾ç¤ºæ–¹æ³•å¯¹æ¯”è¯´æ˜\n",
    "            print(f\"\\nğŸ”„ æ–¹æ³•å¯¹æ¯”è¯´æ˜:\")\n",
    "            print(f\"   æ–¹æ³•1ï¼ˆæ·±è“è‰²ï¼‰: è‡ªé€‚åº”æ´ªæ°´æå– - çŸ©å½¢å†…å±€éƒ¨é˜ˆå€¼ + çŸ©å½¢å¤–å…¨å±€é˜ˆå€¼\")\n",
    "            print(f\"   æ–¹æ³•2ï¼ˆçº¢è‰²ï¼‰: æ··åˆæ ¼ç½‘åˆ†å— - å¤§çŸ©å½¢ç‹¬ç«‹åˆ†å— + å°æ ¼ç½‘è§„åˆ™åˆ’åˆ†\")\n",
    "            print(f\"   å¯é€šè¿‡å›¾å±‚æ§åˆ¶é¢æ¿åˆ‡æ¢æ˜¾ç¤º/éšè—ä¸åŒæ–¹æ³•çš„ç»“æœ\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ æ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {str(e)}\")\n",
    "        print(\"   è¯·æ£€æŸ¥è¾“å…¥æ•°æ®å’Œä»£ç é€»è¾‘\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ å‰ææ¡ä»¶ä¸æ»¡è¶³ï¼Œæ— æ³•æ‰§è¡Œæ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²\")\n",
    "    print(\"   è¯·å…ˆè¿è¡Œå‰é¢ç« èŠ‚çš„ä»£ç ï¼Œç¡®ä¿æ‰€æœ‰å¿…è¦çš„æ•°æ®éƒ½å·²å‡†å¤‡å°±ç»ª\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# æ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²æ¨¡å—æ‰§è¡Œå®Œæ¯•ï¼ˆå·²æ•´åˆåˆ°ç¬¬å…«ç« åœ°å›¾ï¼‰\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "# æ˜¾ç¤ºæ›´æ–°åçš„ç»„åˆåœ°å›¾\n",
    "Map_final_flood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afef4e",
   "metadata": {},
   "source": [
    "# é™„åŠ ä¸€ï¼šæ•°æ®å¯¼å‡º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d88d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é™„åŠ 1ï¼šæ•°æ®å¯¼å‡º - ç¬¬å…«ç« å’Œç¬¬ä¹ç« åˆ†å‰²ç»“æœå¯¼å‡º\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# é™„åŠ 1ï¼šæ•°æ®å¯¼å‡º - ç¬¬å…«ç« å’Œç¬¬ä¹ç« åˆ†å‰²ç»“æœå¯¼å‡º\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "def prepare_and_export_segmentation_results():\n",
    "    \"\"\"\n",
    "    å‡†å¤‡å¹¶å¯¼å‡ºç¬¬å…«ç« å’Œç¬¬ä¹ç« çš„åˆ†å‰²ç»“æœ\n",
    "    ç¡®ä¿åˆ†å‰²ç»“æœçš„ç»Ÿä¸€æ ¼å¼ï¼šæ´ªæ°´åƒç´ =1ï¼Œéæ´ªæ°´åƒç´ =0\n",
    "    \"\"\"\n",
    "    \n",
    "    # æ£€æŸ¥å¿…è¦æ•°æ®çš„å­˜åœ¨æ€§\n",
    "    print(\"\\næ­¥éª¤1: æ£€æŸ¥æ•°æ®å¯ç”¨æ€§\")\n",
    "    data_check = True\n",
    "    \n",
    "    # æ£€æŸ¥ç¬¬å…«ç« ç»“æœ\n",
    "    if 'final_flood_extraction_results' not in globals():\n",
    "        print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ç¬¬å…«ç« çš„æ´ªæ°´æå–ç»“æœ\")\n",
    "        print(\"   è¯·ç¡®ä¿å·²æˆåŠŸè¿è¡Œç¬¬8ç« çš„ä»£ç \")\n",
    "        data_check = False\n",
    "    \n",
    "    # æ£€æŸ¥ç¬¬ä¹ç« ç»“æœ\n",
    "    if 'hybrid_grid_results' not in globals():\n",
    "        print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ç¬¬ä¹ç« çš„æ··åˆæ ¼ç½‘åˆ†å—ç»“æœ\")\n",
    "        print(\"   è¯·ç¡®ä¿å·²æˆåŠŸè¿è¡Œç¬¬9ç« çš„ä»£ç \")\n",
    "        data_check = False\n",
    "    \n",
    "    # æ£€æŸ¥ç ”ç©¶åŒºåŸŸ\n",
    "    if 'roi_choose1' not in globals() or roi_choose1 is None:\n",
    "        print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ç ”ç©¶åŒºåŸŸ\")\n",
    "        data_check = False\n",
    "    \n",
    "    # æ£€æŸ¥ç›®æ ‡æ—¥æœŸ\n",
    "    if 'targdate' not in globals():\n",
    "        print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ç›®æ ‡æ—¥æœŸ\")\n",
    "        data_check = False\n",
    "    \n",
    "    if not data_check:\n",
    "        print(\"âŒ æ•°æ®æ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•æ‰§è¡Œå¯¼å‡º\")\n",
    "        return False\n",
    "    \n",
    "    print(\"âœ… æ•°æ®æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹å‡†å¤‡å¯¼å‡ºæ•°æ®\")\n",
    "    \n",
    "    # === æ­¥éª¤2: å‡†å¤‡ç¬¬å…«ç« çš„åˆ†å‰²ç»“æœ ===\n",
    "    print(\"\\næ­¥éª¤2: å‡†å¤‡ç¬¬å…«ç« è‡ªé€‚åº”æ´ªæ°´æå–ç»“æœ\")\n",
    "    \n",
    "    try:\n",
    "        # è·å–ç¬¬å…«ç« çš„æ´ªæ°´å›¾\n",
    "        method1_flood_map = final_flood_extraction_results['flood_map']\n",
    "        \n",
    "        # ç¡®ä¿ç»“æœä¸ºäºŒå€¼å›¾åƒï¼šæ´ªæ°´=1ï¼Œéæ´ªæ°´=0\n",
    "        # ç”±äºåŸå§‹ç»“æœå¯èƒ½æœ‰æ©è†œï¼Œéœ€è¦æ ‡å‡†åŒ–å¤„ç†\n",
    "        method1_binary = method1_flood_map.gt(0).unmask(0).clip(roi_choose1)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºbyteç±»å‹ä»¥å‡å°æ–‡ä»¶å¤§å°\n",
    "        method1_final = method1_binary.byte()\n",
    "        \n",
    "        print(\"  âœ… ç¬¬å…«ç« ç»“æœå·²å‡†å¤‡å®Œæˆ\")\n",
    "        print(f\"     ç­–ç•¥: {final_flood_extraction_results['extraction_info']['extraction_strategy']}\")\n",
    "        print(f\"     çŸ©å½¢æ•°é‡: {final_flood_extraction_results['extraction_info']['num_rectangles']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ç¬¬å…«ç« ç»“æœå‡†å¤‡å¤±è´¥: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "    # === æ­¥éª¤3: å‡†å¤‡ç¬¬ä¹ç« çš„åˆ†å‰²ç»“æœ ===\n",
    "    print(\"\\næ­¥éª¤3: å‡†å¤‡ç¬¬ä¹ç« æ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²ç»“æœ\")\n",
    "    \n",
    "    try:\n",
    "        # è·å–ç¬¬ä¹ç« çš„åˆ†å‰²å›¾\n",
    "        method2_segmentation_map = hybrid_grid_results['segmentation_map']\n",
    "        \n",
    "        # ç¡®ä¿ç»“æœä¸ºäºŒå€¼å›¾åƒï¼šæ´ªæ°´=1ï¼Œéæ´ªæ°´=0\n",
    "        method2_binary = method2_segmentation_map.gt(0).unmask(0).clip(roi_choose1)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºbyteç±»å‹ä»¥å‡å°æ–‡ä»¶å¤§å°\n",
    "        method2_final = method2_binary.byte()\n",
    "        \n",
    "        print(\"  âœ… ç¬¬ä¹ç« ç»“æœå·²å‡†å¤‡å®Œæˆ\")\n",
    "        print(f\"     ç­–ç•¥: {hybrid_grid_results['segmentation_info']['method']}\")\n",
    "        print(f\"     æ€»åˆ†å—æ•°: {hybrid_grid_results['segmentation_info']['total_large_rectangles'] + hybrid_grid_results['segmentation_info']['total_grid_cells']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ç¬¬ä¹ç« ç»“æœå‡†å¤‡å¤±è´¥: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "    # === æ­¥éª¤4: æ‰¹é‡å¯¼å‡ºè®¾ç½® ===\n",
    "    print(\"\\næ­¥éª¤4: è®¾ç½®æ‰¹é‡å¯¼å‡ºä»»åŠ¡\")\n",
    "    \n",
    "    folder = 'SCI_Valencia_Segmentation_Results'  # Google Driveå¯¼å‡ºæ–‡ä»¶å¤¹\n",
    "    scale = 10  # å¯¼å‡ºåˆ†è¾¨ç‡ï¼ˆç±³ï¼‰\n",
    "    crs = 'EPSG:4326'  # åæ ‡ç³»ç»Ÿ\n",
    "    max_pixels = 1e13  # æœ€å¤§åƒç´ æ•°\n",
    "    \n",
    "    # åˆ›å»ºå¯¼å‡ºä»»åŠ¡åˆ—è¡¨\n",
    "    export_tasks = []\n",
    "    \n",
    "    # ä»»åŠ¡1: å¯¼å‡ºç¬¬å…«ç« ç»“æœ\n",
    "    task1 = ee.batch.Export.image.toDrive(\n",
    "        image=method1_final,\n",
    "        description=f'Method1_AdaptiveFloodExtraction_{targdate}',\n",
    "        folder=folder,\n",
    "        fileNamePrefix=f'Method1_AdaptiveFloodExtraction_{targdate}',\n",
    "        scale=scale,\n",
    "        region=roi_choose1,\n",
    "        crs=crs,\n",
    "        maxPixels=max_pixels\n",
    "    )\n",
    "    export_tasks.append(('æ–¹æ³•1-è‡ªé€‚åº”æ´ªæ°´æå–', task1))\n",
    "    \n",
    "    # ä»»åŠ¡2: å¯¼å‡ºç¬¬ä¹ç« ç»“æœ\n",
    "    task2 = ee.batch.Export.image.toDrive(\n",
    "        image=method2_final,\n",
    "        description=f'Method2_HybridGridSegmentation_{targdate}',\n",
    "        folder=folder,\n",
    "        fileNamePrefix=f'Method2_HybridGridSegmentation_{targdate}',\n",
    "        scale=scale,\n",
    "        region=roi_choose1,\n",
    "        crs=crs,\n",
    "        maxPixels=max_pixels\n",
    "    )\n",
    "    export_tasks.append(('æ–¹æ³•2-æ··åˆæ ¼ç½‘åˆ†å—', task2))\n",
    "    \n",
    "    # === æ­¥éª¤5: å¯åŠ¨æ‰€æœ‰å¯¼å‡ºä»»åŠ¡ ===\n",
    "    print(f\"\\næ­¥éª¤5: å¯åŠ¨å¯¼å‡ºä»»åŠ¡\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    successful_exports = 0\n",
    "    failed_exports = 0\n",
    "    \n",
    "    for task_name, task in export_tasks:\n",
    "        try:\n",
    "            task.start()\n",
    "            print(f\"  âœ… {task_name} å¯¼å‡ºä»»åŠ¡å·²å¯åŠ¨\")\n",
    "            successful_exports += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {task_name} å¯¼å‡ºä»»åŠ¡å¯åŠ¨å¤±è´¥: {str(e)}\")\n",
    "            failed_exports += 1\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # === æ­¥éª¤6: å¯¼å‡ºæ‘˜è¦ä¿¡æ¯ ===\n",
    "    print(f\"\\nğŸ“Š å¯¼å‡ºä»»åŠ¡æ‘˜è¦:\")\n",
    "    print(f\"  âœ… æˆåŠŸå¯åŠ¨: {successful_exports}/{len(export_tasks)} ä¸ªä»»åŠ¡\")\n",
    "    print(f\"  âŒ å¯åŠ¨å¤±è´¥: {failed_exports}/{len(export_tasks)} ä¸ªä»»åŠ¡\")\n",
    "    print(f\"  ğŸ“ å¯¼å‡ºæ–‡ä»¶å¤¹: {folder}\")\n",
    "    print(f\"  ğŸ—“ï¸ ç›®æ ‡æ—¥æœŸ: {targdate}\")\n",
    "    print(f\"  ğŸ“ åˆ†è¾¨ç‡: {scale}ç±³\")\n",
    "    print(f\"  ğŸ—ºï¸ åæ ‡ç³»: {crs}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ å¯¼å‡ºæ–‡ä»¶æ¸…å•:\")\n",
    "    print(f\"  1. Method1_AdaptiveFloodExtraction_{targdate}.tif\")\n",
    "    print(f\"     - ç¬¬å…«ç« è‡ªé€‚åº”æ´ªæ°´æå–ç»“æœï¼ˆäºŒå€¼å›¾ï¼š1=æ´ªæ°´ï¼Œ0=éæ´ªæ°´ï¼‰\")\n",
    "    print(f\"  2. Method2_HybridGridSegmentation_{targdate}.tif\")\n",
    "    print(f\"     - ç¬¬ä¹ç« æ··åˆæ ¼ç½‘åˆ†å—åˆ†å‰²ç»“æœï¼ˆäºŒå€¼å›¾ï¼š1=æ´ªæ°´ï¼Œ0=éæ´ªæ°´ï¼‰\")\n",
    "    \n",
    "    print(f\"\\nğŸ” åƒç´ å€¼è¯´æ˜:\")\n",
    "    print(f\"  â€¢ æ‰€æœ‰ç»“æœå›¾åƒï¼š1 = æ´ªæ°´/æš—ç›®æ ‡ï¼Œ0 = éæ´ªæ°´\")\n",
    "    print(f\"  â€¢ æ•°æ®ç±»å‹ï¼š8ä½æ— ç¬¦å·æ•´å‹ï¼ˆByteï¼‰\")\n",
    "    print(f\"  â€¢ æ— æ•°æ®å€¼ï¼š0ï¼ˆéæ´ªæ°´åŒºåŸŸï¼‰\")\n",
    "    \n",
    "    print(f\"\\nâ³ æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€:\")\n",
    "    print(f\"  â€¢ è®¿é—® https://code.earthengine.google.com\")\n",
    "    print(f\"  â€¢ ç‚¹å‡» 'Tasks' æ ‡ç­¾é¡µæŸ¥çœ‹å¯¼å‡ºè¿›åº¦\")\n",
    "    print(f\"  â€¢ ä»»åŠ¡å®Œæˆåæ–‡ä»¶å°†å‡ºç°åœ¨Google Driveçš„ '{folder}' æ–‡ä»¶å¤¹ä¸­\")\n",
    "    \n",
    "    # ä¿å­˜å¯¼å‡ºä¿¡æ¯\n",
    "    global segmentation_export_info\n",
    "    segmentation_export_info = {\n",
    "        'folder': folder,\n",
    "        'target_date': targdate,\n",
    "        'successful_tasks': successful_exports,\n",
    "        'failed_tasks': failed_exports,\n",
    "        'total_tasks': len(export_tasks),\n",
    "        'export_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'files': [\n",
    "            f'Method1_AdaptiveFloodExtraction_{targdate}.tif',\n",
    "            f'Method2_HybridGridSegmentation_{targdate}.tif'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return successful_exports > 0\n",
    "\n",
    "# æ‰§è¡Œå¯¼å‡ºæµç¨‹\n",
    "print(\"å¼€å§‹æ‰§è¡Œåˆ†å‰²ç»“æœå¯¼å‡ºæµç¨‹...\")\n",
    "\n",
    "if prepare_and_export_segmentation_results():\n",
    "    print(\"\\nğŸ‰ åˆ†å‰²ç»“æœå¯¼å‡ºæµç¨‹æ‰§è¡ŒæˆåŠŸï¼\")\n",
    "    print(\"   âœ“ æ´ªæ°´åˆ†å‰²ç»“æœå¯¼å‡ºä»»åŠ¡å·²æäº¤åˆ°Google Earth Engine\")\n",
    "    print(\"   âœ“ è¯·è€å¿ƒç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œå¤§æ–‡ä»¶å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶\")\n",
    "    print(\"   âœ“ å¯¼å‡ºä¿¡æ¯å·²ä¿å­˜åˆ°å…¨å±€å˜é‡ 'segmentation_export_info'\")\n",
    "else:\n",
    "    print(\"\\nâŒ åˆ†å‰²ç»“æœå¯¼å‡ºæµç¨‹æ‰§è¡Œå¤±è´¥\")\n",
    "    print(\"   è¯·æ£€æŸ¥å‰é¢ç« èŠ‚çš„è¿è¡Œç»“æœï¼Œç¡®ä¿ç¬¬å…«ç« å’Œç¬¬ä¹ç« éƒ½å·²æˆåŠŸå®Œæˆ\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# é™„åŠ 1ï¼šæ•°æ®å¯¼å‡ºæ¨¡å—æ‰§è¡Œå®Œæ¯•\")\n",
    "print(\"#\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
